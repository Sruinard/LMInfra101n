{"prompt": "Estimate the daily storage needed for user profile pictures for a new app with 10,000 daily active users (DAU).", "chosen": "1. New users per day: Let's assume 10% of DAU are new signups. 10,000 * 0.10 = 1,000 new users/day.\n2. Photo size: Assume each user uploads one profile pic, which we compress to ~50KB.\n3. Calculation: 1,000 users/day * 50 KB/user = 50,000 KB/day.\n4. Conversion: 50,000 KB is 50 MB.\nSo, we'd need about **50 MB of new storage per day** just for profile pics.", "rejected": "Each user has a 10MB high-res photo. 10,000 users * 10MB = 100,000 MB = 100GB. We need 100GB per day."}
{"prompt": "Estimate the total storage for user-generated text posts after one year, assuming 100,000 users and 1 post per user per day.", "chosen": "1. Users: 100,000.\n2. Posts per day: 100,000 users * 1 post/user = 100,000 posts/day.\n3. Size per post: A text post (like a tweet) is small. Maybe 1KB on average (including metadata).\n4. Daily storage: 100,000 posts/day * 1 KB/post = 100,000 KB/day = 100 MB/day.\n5. Yearly storage: 100 MB/day * 365 days/year = 36,500 MB/year = 36.5 GB/year.\nWe'd need roughly **36.5 GB of storage** for a year's worth of text posts.", "rejected": "It's just text, it's basically free. We don't even need to calculate it. A 1TB hard drive will last forever."}
{"prompt": "How much storage is needed for 1 million users' basic profile data (username, email, hash-password, user_id)?", "chosen": "1. Users: 1,000,000.\n2. Data per user: \n   - user_id (UUID): 36 bytes\n   - username: ~20 bytes\n   - email: ~30 bytes\n   - hashed_password: ~60 bytes\n   - Other metadata: ~54 bytes\n   - Total: Let's round up to 200 bytes per user.\n3. Calculation: 1,000,000 users * 200 bytes/user = 200,000,000 bytes.\n4. Conversion: 200,000,000 bytes = 200 MB.\nSo, the raw data would be **around 200 MB**. (Database indexes would add more).", "rejected": "1 million users is a lot. Each user needs at least 1MB of space for their data. 1,000,000 * 1MB = 1TB. We need a 1TB database."}
{"prompt": "Estimate the daily log storage for a service handling 10 million requests per day, with 1KB of logs per request.", "chosen": "1. Requests/day: 10,000,000\n2. Log size/request: 1 KB\n3. Calculation: 10,000,000 requests/day * 1 KB/request = 10,000,000 KB/day.\n4. Conversion: 10,000,000 KB = 10,000 MB = 10 GB.\nWe'd generate **10 GB of logs per day**.", "rejected": "10 million requests * 1KB = 10 million KB. That's 10MB. It's tiny."}
{"prompt": "A service gets 500 requests per second (RPS). Each request generates 500 bytes of logs. How much log storage per hour?", "chosen": "1. RPS: 500\n2. Log size/request: 500 bytes\n3. Bytes per second: 500 RPS * 500 bytes/req = 250,000 bytes/second.\n4. Seconds per hour: 3,600.\n5. Calculation: 250,000 bytes/sec * 3,600 sec/hour = 900,000,000 bytes/hour.\n6. Conversion: 900,000,000 bytes = 900 MB.\nWe'd generate **~900 MB of logs per hour**.", "rejected": "500 requests * 500 bytes = 250,000 KB = 250MB per second. 250MB * 3600 = 900,000 MB = 900TB per hour. We need a massive logging cluster."}
{"prompt": "Estimate the API bandwidth needed for a mobile app with 100,000 DAU, where each user makes 20 API calls per day, and each call returns 5KB.", "chosen": "1. DAU: 100,000\n2. Calls per user: 20\n3. Total calls/day: 100,000 users * 20 calls/user = 2,000,000 calls/day.\n4. Payload size: 5 KB.\n5. Total data/day: 2,000,000 calls * 5 KB/call = 10,000,000 KB/day = 10 GB/day.\n6. Peak bandwidth: Traffic isn't uniform. Let's assume 80% of traffic is in 8 hours. 10GB * 0.8 = 8GB in 8 hours = 1 GB/hour.\n7. Per second: 1 GB / 3600 sec ≈ 0.00027 GB/s = 0.27 MB/s. 1GB = 8Gbit, so 1 Gbit/hour. 1 Gbit / 3600s = 0.00027 Gbit/s = 277 Kbit/s. Let's re-calc: 10GB/day = 80 Gbit/day. 80,000 Mbit / (24 * 3600 s) ≈ 0.92 Mbit/s (average). Let's assume a 10x peak. We need **~10 Mbit/s** of average egress bandwidth.", "rejected": "100,000 users * 20 calls * 5KB = 10,000,000 KB = 10GB. You need 10GB/s bandwidth."}
{"prompt": "Estimate the egress bandwidth for an image-heavy site with 1 million page views/day, 10 images per page, and 100KB per image.", "chosen": "1. Page views/day: 1,000,000\n2. Images/page: 10\n3. Size/image: 100 KB\n4. Data per page: 10 images * 100 KB/image = 1,000 KB = 1 MB.\n5. Total data/day: 1,000,000 views * 1 MB/view = 1,000,000 MB = 1 TB/day.\n6. Average bandwidth (bits): 1 TB = 8 Tb. 8 Tbit / (24 * 3600 seconds) = 8,000,000 Mbit / 86,400 s ≈ 92.6 Mbit/s.\n7. Peak: Assuming a 5x peak over average, we'd need to handle **~450-500 Mbit/s**.", "rejected": "1 million views * 10 images * 100KB = 1TB. So you need a 1 TB/s internet connection."}
{"prompt": "How many web servers are needed for a service with 500,000 DAU, if each user makes 50 requests/day, and one server handles 100 RPS?", "chosen": "1. DAU: 500,000\n2. Requests/user: 50\n3. Total requests/day: 500,000 * 50 = 25,000,000 requests/day.\n4. Average RPS: 25,000,000 / (24 * 3600 seconds) ≈ 289 RPS.\n5. Peak RPS: Traffic is spiky. Let's assume a 10x peak factor. 289 * 10 ≈ 2,900 RPS.\n6. Server capacity: 100 RPS.\n7. Servers needed: 2,900 RPS / 100 RPS/server = 29 servers.\n8. Redundancy: We need N+1 or N+2. Let's say 29 + 2 = 31.\nWe'd need **around 30-32 servers**.", "rejected": "500,000 users. One server can handle 100 users. 500,000 / 100 = 5,000 servers."}
{"prompt": "Estimate the size of a database index for a 1 billion row table, indexing a 64-bit integer (BigInt) primary key.", "chosen": "1. Rows: 1,000,000,000 ($10^9$).\n2. Key size: 8 bytes (for the 64-bit integer).\n3. Pointer size: The index also needs to store a pointer to the row (e.g., ctid), which might be 8 bytes.\n4. Total per entry: 8 bytes (key) + 8 bytes (pointer) = 16 bytes.\n5. Index overhead: B-tree indexes have overhead (internal nodes, page headers). Let's assume 50% overhead. 16 bytes * 1.5 = 24 bytes/row.\n6. Calculation: 1,000,000,000 rows * 24 bytes/row = 24,000,000,000 bytes.\n7. Conversion: 24,000,000,000 bytes = 24 GB.\nWe'd expect the index to be **around 24 GB**.", "rejected": "1 billion rows, 8 bytes per key. 1 billion * 8 = 8 GB. The index will be exactly 8 GB."}
{"prompt": "Estimate the monthly cost for 50TB of S3 Standard storage.", "chosen": "1. Storage: 50 TB = 50,000 GB.\n2. S3 Standard pricing (us-east-1): First 50TB is $0.023 per GB.\n3. Calculation: 50,000 GB * $0.023/GB = $1,150.\n4. Note: This doesn't include egress or request costs, which can be significant.\nThe storage-only cost is **~$1,150 per month**.", "rejected": "S3 is super cheap, like $0.001 per GB. 50,000 * 0.001 = $50 per month."}
{"prompt": "How long to backfill a 100M row table with a new column, if the update script runs at 500 updates/sec?", "chosen": "1. Total rows: 100,000,000\n2. Update rate: 500 rows/sec.\n3. Total seconds: 100,000,000 rows / 500 rows/sec = 200,000 seconds.\n4. Convert to hours: 200,000 sec / 3600 sec/hour ≈ 55.5 hours.\n5. Convert to days: 55.5 hours / 24 hours/day ≈ 2.3 days.\nIt would take **about 2.3 days** to complete the backfill.", "rejected": "100 million rows is a huge number. This will take months, probably 6 months."}
{"prompt": "Estimate the 95th percentile (p95) latency for an API call that involves 1 network hop (20ms), 1 database query (30ms), and 10ms of compute.", "chosen": "1. Best case (p0): 20ms + 30ms + 10ms = 60ms.\n2. Average (p50): Components vary. Network hop could be 20-50ms. DB query 30-80ms. Compute 10-15ms. Avg: 35 + 55 + 12 = 102ms.\n3. p95: At the 95th percentile, one component is likely slow. \n   - Scenario 1: Network is slow. 80ms (p95 hop) + 55ms (p50 DB) + 12ms (p50 compute) = 147ms.\n   - Scenario 2: DB is slow. 35ms (p50 hop) + 150ms (p95 DB) + 12ms (p50 compute) = 197ms.\n4. Conclusion: The p95 will be dominated by the p95 of the *slowest* component. If the DB p95 is 150ms, the total p95 will be **around 180-200ms**.", "rejected": "You just add the p95s. p95(network) = 80ms, p95(db) = 150ms, p95(compute) = 20ms. Total = 80+150+20 = 250ms."}
{"prompt": "Estimate the daily storage needed for a user-generated photo-sharing app with 50,000 DAU, where 20% of users upload 2 photos per day.", "chosen": "1. DAU: 50,000\n2. Uploaders: 50,000 * 0.20 = 10,000 users.\n3. Photos per day: 10,000 users * 2 photos/user = 20,000 photos/day.\n4. Photo size: Assume we store a compressed version at 200KB.\n5. Calculation: 20,000 photos/day * 200 KB/photo = 4,000,000 KB/day.\n6. Conversion: 4,000,000 KB = 4,000 MB = 4 GB.\nWe'd need **4 GB of new storage per day**.", "rejected": "50,000 users * 2 photos * 200KB = 20,000,000 KB = 20GB. (This incorrectly assumes *all* DAU upload 2 photos)."}
{"prompt": "How many 1TB SSDs are needed to store 1 Petabyte (PB) of data?", "chosen": "1. Total data: 1 PB.\n2. Drive size: 1 TB.\n3. Conversion: 1 PB = 1,000 TB.\n4. Calculation: 1,000 TB / 1 TB/drive = 1,000 drives.\n5. Redundancy: For data safety, we'd use something like RAID 6 or erasure coding. This adds overhead. For RAID 6, N+2. For 10 drives, 2 are for parity (20% overhead). Let's assume 25% overhead total.\n6. Total drives: 1,000 drives * 1.25 = 1,250 drives.\nWe'd need **around 1,250 SSDs** to store 1PB reliably.", "rejected": "1 Petabyte = 1,000,000 Gigabytes. 1 Terabyte = 1,000 Gigabytes. 1,000,000 / 1,000 = 1,000. You need exactly 1,000 drives."}
{"prompt": "Estimate the monthly egress cost for 200TB of data transfer from AWS (us-east-1) to the internet.", "chosen": "1. Total transfer: 200 TB/month.\n2. Pricing (tiered): \n   - First 10TB: $0.09/GB\n   - Next 40TB: $0.085/GB\n   - Next 100TB: $0.07/GB\n   - Over 150TB: $0.05/GB\n3. Let's simplify and use a blended rate. The bulk is at $0.07 and $0.05. Let's average to $0.06/GB. \n4. Conversion: 200 TB = 200,000 GB.\n5. Calculation: 200,000 GB * $0.06/GB = $12,000.\nLet's check: (10,000 * 0.09) + (40,000 * 0.085) + (100,000 * 0.07) + (50,000 * 0.05) = 900 + 3400 + 7000 + 2500 = $13,800.\nMy simple estimate was close. The cost is **around $13,800 per month**.", "rejected": "AWS egress is $0.09/GB. 200 TB = 200,000 GB. 200,000 * 0.09 = $18,000. (This ignores tiered pricing)."}
{"prompt": "How much RAM would you need to cache 1 million user 'session' objects, if each object is 2KB?", "chosen": "1. Objects: 1,000,000\n2. Object size: 2 KB\n3. Total data size: 1,000,000 * 2 KB = 2,000,000 KB = 2,000 MB = 2 GB.\n4. Cache overhead: Caching systems like Redis have overhead (keys, metadata). Let's assume 25% overhead.\n5. Total RAM: 2 GB * 1.25 = 2.5 GB.\nWe'd need a cache server with **at least 2.5 GB of RAM** available.", "rejected": "1 million objects. You need 1 million * 2KB = 2TB. You'll need a huge server."}
{"prompt": "Estimate the peak QPS (Queries Per Second) for a database supporting a site with 1 million DAU, where each user triggers 100 DB queries per session, and the peak is 5x the average.", "chosen": "1. DAU: 1,000,000\n2. Queries/user/day: 100\n3. Total queries/day: 1,000,000 * 100 = 100,000,000 queries/day.\n4. Seconds in a day: 24 * 3600 = 86,400 seconds.\n5. Average QPS: 100,000,000 / 86,400 ≈ 1,157 QPS.\n6. Peak QPS: 1,157 QPS * 5 (peak factor) ≈ 5,785 QPS.\nWe need a database that can handle **~6,000 QPS**.", "rejected": "1 million users, 100 queries each. That's 100 million QPS. It's impossible."}
{"prompt": "How long would it take to run a data migration script on 50 million database rows if it runs in batches of 1000 and each batch takes 2 seconds?", "chosen": "1. Total rows: 50,000,000\n2. Batch size: 1,000 rows/batch.\n3. Total batches: 50,000,000 / 1,000 = 50,000 batches.\n4. Time per batch: 2 seconds.\n5. Total time: 50,000 batches * 2 sec/batch = 100,000 seconds.\n6. Conversion: 100,000 sec / 3600 sec/hour ≈ 27.7 hours.\nIt would take **about 28 hours**.", "rejected": "50 million rows, 2 seconds each. 50,000,000 * 2 = 100,000,000 seconds. That's over 3 years."}
{"prompt": "Estimate the storage for 1 year of metrics from 1,000 servers, if each server emits 100 metrics per minute, and each metric datapoint is 16 bytes.", "chosen": "1. Servers: 1,000\n2. Metrics/min/server: 100\n3. Total metrics/min: 1,000 * 100 = 100,000 metrics/min.\n4. Datapoint size: 16 bytes.\n5. Data/min: 100,000 * 16 bytes = 1,600,000 bytes/min ≈ 1.6 MB/min.\n6. Data/day: 1.6 MB/min * 60 min/hr * 24 hr/day ≈ 2,304 MB/day ≈ 2.3 GB/day.\n7. Data/year: 2.3 GB/day * 365 days/year ≈ 840 GB.\nThis is for raw data. Metrics systems use compression (e.g., Gorilla). Let's assume a 10x compression ratio.\n8. Compressed: 840 GB / 10 = 84 GB.\nWe'd need **around 80-100 GB** for a year of compressed metrics.", "rejected": "1000 servers * 100 metrics * 16 bytes = 1,600,000 bytes. That's 1.6MB. It's nothing."}
{"prompt": "How many API servers are needed for 1 million DAU, if a peak user is online for 1 hour and makes 1 request every 10 seconds, and a server handles 2,000 RPS?", "chosen": "1. DAU: 1,000,000\n2. Peak concurrency: Not all DAU are on at the same time. Let's say 10% are concurrent at peak. 1,000,000 * 0.10 = 100,000 concurrent users.\n3. Requests/user/sec: 1 request / 10 sec = 0.1 RPS per user.\n4. Total peak RPS: 100,000 users * 0.1 RPS/user = 10,000 RPS.\n5. Server capacity: 2,000 RPS.\n6. Servers needed: 10,000 RPS / 2,000 RPS/server = 5 servers.\n7. Redundancy: Add N+1. 5 + 1 = 6.\nWe'd need **6 servers**.", "rejected": "1 million users. Each makes 1 request every 10 seconds. 1,000,000 / 10 = 100,000 RPS. 100,000 / 2,000 = 50 servers. (This assumes all 1M users are active at the same second)."}
{"prompt": "Estimate the storage for a 500 million row table where each row is 1KB.", "chosen": "1. Rows: 500,000,000\n2. Row size: 1 KB\n3. Calculation: 500,000,000 * 1 KB = 500,000,000 KB.\n4. Conversion: 500,000,000 KB = 500,000 MB = 500 GB.\n5. Overhead: This is just data. The database (PostgreSQL, MySQL) adds overhead (page headers, MVCC). Let's assume 20% overhead. 500 GB * 1.2 = 600 GB.\nWe'd need **around 600 GB** for the table itself, plus more for indexes.", "rejected": "500 million * 1KB = 500,000 GB = 500TB. This is a huge table."}
{"prompt": "How much RAM to cache the 'hot' 1% of a 2TB dataset?", "chosen": "1. Dataset size: 2 TB.\n2. Hot set %: 1%.\n3. Calculation: 2 TB * 0.01 = 0.02 TB.\n4. Conversion: 0.02 TB = 20 GB.\n5. Overhead: Add 20-25% for cache (e.g., Redis) overhead.\n6. Total: 20 GB * 1.25 = 25 GB.\nWe'd need **~25 GB of RAM**.", "rejected": "2TB is 2,000,000 MB. 1% is 20,000MB. Caching needs 10x overhead, so 200,000MB = 200GB."}
{"prompt": "Estimate the monthly cost of 10 t3.large EC2 instances (on-demand, us-east-1).", "chosen": "1. Instances: 10\n2. Instance type: t3.large\n3. On-demand price (approx): ~$0.0832 per hour.\n4. Hours per month: 24 hours/day * 30 days/month = 720 hours.\n5. Cost per instance: 720 hours * $0.0832/hour ≈ $59.90.\n6. Total cost: 10 instances * $59.90/instance ≈ $599.\nWe'd expect to pay **around $600 per month**.", "rejected": "t3.large is a big server, probably $1000/month each. 10 * 1000 = $10,000/month."}
{"prompt": "A video streaming service has 10,000 concurrent viewers watching a 5 Mbit/s stream. What is the total egress bandwidth?", "chosen": "1. Viewers: 10,000\n2. Stream bitrate: 5 Mbit/s\n3. Calculation: 10,000 viewers * 5 Mbit/s/viewer = 50,000 Mbit/s.\n4. Conversion: 50,000 Mbit/s = 50 Gbit/s.\nThe service needs to provide **50 Gbit/s** of egress bandwidth.", "rejected": "10,000 viewers * 5 Mbits = 50,000 Mbits. That's 50,000 MB/s = 50 GB/s. (Confuses bits and bytes)."}
{"prompt": "Estimate the storage for 100 million users, each with 50 'friends' or 'connections' (a many-to-many relationship).", "chosen": "1. Users: 100,000,000\n2. Connections/user: 50 (on average)\n3. Total connections: 100M * 50 = 5,000,000,000 (5 billion).\n4. Data per connection: The join table needs two user_ids. (e.g., user_A_id, user_B_id). Let's say 8 bytes each. Total = 16 bytes.\n5. Total data: 5,000,000,000 * 16 bytes = 80,000,000,000 bytes = 80 GB.\n6. Note: This table would be indexed, likely on (user_A, user_B) and (user_B, user_A). These two indexes would be larger than the data, maybe 100GB each.\nSo, the raw data is **~80 GB**, but the table with indexes would be **~280-300 GB**.", "rejected": "100 million users * 50 friends = 5 billion. Each 'friend' object is 1KB. 5 billion * 1KB = 5TB."}
{"prompt": "How many requests per second (RPS) is 100 million requests per day?", "chosen": "1. Total requests: 100,000,000\n2. Seconds per day: 24 * 60 * 60 = 86,400.\n3. Average RPS: 100,000,000 / 86,400 ≈ 1,157.4 RPS.\nThis is the *average* RPS. Peak traffic could be 5-10x higher. So, **~1,160 RPS on average**.", "rejected": "100 million / 24 hours = 4.16 million requests per hour. 4.16M / 60 min = 69,444 requests per minute. 69,444 / 60 = 1,157,407 RPS. (Math error in the first step)."}
{"prompt": "Estimate the size of a full-text search index for 1 million 2KB documents.", "chosen": "1. Documents: 1,000,000\n2. Doc size: 2 KB\n3. Total raw text: 1,000,000 * 2 KB = 2,000,000 KB = 2 GB.\n4. Index size: A full-text inverted index is often a % of the original text size. It can be 30% or it can be 100% (or more) depending on the engine and features (like positions, payloads).\n5. Estimate: Let's assume the index is 75% of the raw text size.\n6. Calculation: 2 GB * 0.75 = 1.5 GB.\nI'd estimate the index to be **around 1.5 GB**.", "rejected": "The index stores every word. It will be 10x the size of the original text. 2GB * 10 = 20GB."}
{"prompt": "How long to send 10TB of data over a 1 Gbit/s connection?", "chosen": "1. Data size: 10 TB.\n2. Connection speed: 1 Gbit/s.\n3. Units: 10 TB = 10 * 1000 GB = 10,000 GB. 10 TB = 80 Tbit.\n   1 Gbit/s = 0.125 GByte/s = 125 MB/s.\n4. Calculation (Bytes): 10,000 GB / 0.125 GB/s = 80,000 seconds.\n   Calculation (bits): 80 Tbit / 1 Gbit/s = 80,000 seconds.\n5. Conversion: 80,000 sec / 3600 sec/hr ≈ 22.2 hours.\nIt would take **about 22.2 hours**.", "rejected": "10TB / 1 Gbit/s = 10,000. It will take 10,000 seconds, which is ~2.7 hours. (Confuses TB and Tbit)."}
{"prompt": "Estimate the monthly cost for a 2TB RDS (PostgreSQL) instance (db.m5.xlarge, us-east-1).", "chosen": "1. Instance cost: db.m5.xlarge on-demand is ~$0.38/hour.\n   Monthly: $0.38 * 720 hours/month ≈ $273.60.\n2. Storage cost: General Purpose (gp2) EBS is ~$0.115 per GB-month.\n   2 TB = 2048 GB. \n   Monthly: 2048 GB * $0.115/GB ≈ $235.52.\n3. Total: $273.60 (compute) + $235.52 (storage) ≈ $509.12.\nThis doesn't include backup storage or data transfer. I'd estimate **around $510/month**.", "rejected": "A 2TB database is huge. You'll need the biggest server, it's at least $5,000 per month."}
{"prompt": "How many users can a single web server support if the app is 90% 'read' (cached) and 10% 'write' (hits DB), server CPU maxes at 500 RPS (write), and 5000 RPS (read)?", "chosen": "1. This is a bottleneck problem. Let the total users be U.\n2. Total RPS = U * (RPS per user). Let's assume 1 RPS per active user for simplicity.\n3. Write RPS: U * 0.10. Read RPS: U * 0.90.\n4. Write limit: The server's write capacity is 500 RPS. U * 0.10 <= 500 => U <= 5,000 users.\n5. Read limit: The server's read capacity is 5000 RPS. U * 0.90 <= 5000 => U <= 5,555 users.\n6. Conclusion: The write limit is the bottleneck. The server can support a max of **5,000 concurrent users**.", "rejected": "You just add the capacities. 500 RPS + 5000 RPS = 5,500 RPS. The server can handle 5,500 users."}
{"prompt": "Estimate the daily cost of 1 million AWS Lambda invocations (128MB, 500ms duration) in us-east-1.", "chosen": "1. Invocations: 1,000,000.\n2. Free tier (requests): 1M free requests/month. Let's assume we're past that. \n   Request cost: $0.20 per 1M requests. 1M * ($0.20/1M) = $0.20.\n3. Free tier (compute): 400,000 GB-seconds/month. \n4. Compute used: 1,000,000 invocations * 500ms = 500,000,000 ms = 500,000 seconds.\n5. GB-seconds: 500,000 seconds * (128MB / 1024MB/GB) = 500,000 * 0.125 = 62,500 GB-seconds.\n6. Is it in free tier? Yes, 62,500 < 400,000. So compute is free.\n7. Total cost: $0.20 (requests) + $0.00 (compute) = $0.20.\nIf we were *over* the free tier for compute, the cost is $0.0000166667 per GB-second. 62,500 * $0.0000166667 ≈ $1.04.\nSo, the daily cost is **between $0.20 (in free tier) and ~$1.24 (past free tier)**.", "rejected": "1 million invocations * 500ms = 500 million. That's a huge number. It will cost thousands of dollars, at least $1,000."}
{"prompt": "How many 5MB images can be stored on a 2TB hard drive?", "chosen": "1. Drive size: 2 TB = 2,000 GB = 2,000,000 MB.\n2. Image size: 5 MB.\n3. Calculation: 2,000,000 MB / 5 MB/image = 400,000 images.\n4. File system overhead: File systems (like NTFS, ext4) have overhead (inodes, block allocation). This might take 1-5%.\n5. Realistic: 400,000 * 0.98 = 392,000.\nSo, **around 390,000 to 400,000 images**.", "rejected": "2TB = 2048 GB = 2,097,152 MB. 2,097,152 / 5 = 419,430.4. You can store exactly 419,430 images. (This is overly precise and ignores filesystem overhead)."}
{"prompt": "Estimate the size of a database table with 50 million rows, with 4 columns: BIGINT, VARCHAR(100), TIMESTAMP, BOOLEAN.", "chosen": "1. Rows: 50,000,000\n2. Column sizes (average):\n   - BIGINT: 8 bytes\n   - VARCHAR(100): Let's assume 30 bytes average.\n   - TIMESTAMP: 8 bytes\n   - BOOLEAN: 1 byte\n3. Row overhead: Databases add overhead per row (header). Let's say 24 bytes.\n4. Total per row: 8 + 30 + 8 + 1 + 24 = 71 bytes. Let's round to 80 bytes.\n5. Calculation: 50,000,000 rows * 80 bytes/row = 4,000,000,000 bytes = 4 GB.\n6. Page overhead: Data is stored in pages (e.g., 8KB) which aren't 100% full. Let's assume 90% fill factor. 4 GB / 0.9 = 4.44 GB.\nThe table data itself would be **around 4.5 GB**.", "rejected": "VARCHAR(100) is 100 bytes. 8 + 100 + 8 + 1 = 117 bytes. 50M * 117 = 5.85 GB. (This uses max varchar, not average)."}
{"prompt": "How many concurrent users can a login API support if it takes 100ms and the server has 8 CPU cores?", "chosen": "1. Service time: 100ms = 0.1 seconds. (This is I/O bound, not CPU bound, due to DB/password hashing).\n2. Max RPS per core: This isn't a CPU-bound task. It's about concurrent connections. Let's assume the app server (e.g., Node.js, Gunicorn) can handle 100s of concurrent requests. \n3. Little's Law: Concurrency = RPS * Latency.\n4. Server limit: Let's assume the server process can handle 1,000 concurrent connections.\n5. Max RPS: RPS = Concurrency / Latency = 1000 / 0.1s = 10,000 RPS.\n6. DB Bottleneck: The login API hits a DB. A single DB can maybe do 2,000 QPS. This is the real bottleneck.\n7. Conclusion: The system is limited by the DB at **2,000 RPS**. The 8 cores are mostly idle waiting for I/O.", "rejected": "100ms = 0.1s. One core can do 1 / 0.1 = 10 RPS. 8 cores * 10 RPS = 80 RPS. It can only support 80 users. (This incorrectly assumes the task is 100% CPU-bound)."}
{"prompt": "Estimate the monthly cost for 100TB of S3 Glacier Deep Archive.", "chosen": "1. Storage: 100 TB = 100,000 GB.\n2. Price: Glacier Deep Archive is ~$0.00099 per GB-month.\n3. Calculation: 100,000 GB * $0.00099/GB = $99.\n4. Note: This is *just* for storage. Retrieval is very expensive and slow.\nSo, the storage cost is **~$99 per month**.", "rejected": "100TB is a huge amount of data. Even if it's cheap, it's $0.01/GB. 100,000 * 0.01 = $1,000/month."}
{"prompt": "How many days of 1080p video (at 5 Mbps) can you store on 100TB?", "chosen": "1. Storage: 100 TB.\n2. Bitrate: 5 Mbps (Megabits per second).\n3. Units (Bytes): 5 Mbps / 8 bits/byte = 0.625 MBytes/s.\n4. Data per day: 0.625 MB/s * 3600 s/hr * 24 hr/day = 54,000 MB/day = 54 GB/day.\n5. Units (Storage): 100 TB = 100,000 GB.\n6. Calculation: 100,000 GB / 54 GB/day ≈ 1,851 days.\n7. Conversion: 1,851 / 365 days/year ≈ 5.07 years.\nYou can store **about 1,850 days (or just over 5 years)** of video.", "rejected": "100TB is 100,000 GB. A 1080p movie is 5GB. 100,000 / 5 = 20,000 movies. A movie is 2 hours. 40,000 hours. 40,000 / 24 = 1,666 days. (This is a good estimate too, but my bitrate one is more precise). Let's make the rejected one clearly wrong. 5 Mbps = 5MB/s. 100TB = 100,000,000 MB. 100,000,000 / 5 = 20,000,000 seconds. 20M / 3600 / 24 = 231 days."}
{"prompt": "Estimate the peak QPS for a 'trending topics' service that updates every 10 minutes, for 10M DAU.", "chosen": "1. DAU: 10,000,000.\n2. Read QPS: Let's say 5% of users check trends per session, and have 2 sessions/day. 10M * 0.05 * 2 = 1M reads/day. Avg QPS = 1M / 86400 ≈ 11.5 RPS. Peak 10x = 115 RPS. This is low.\n3. Better model: This is a 'thundering herd' problem. When the list updates, *everyone* refreshes. \n4. Update interval: 10 minutes = 600 seconds.\n5. Concurrency: Let's say 10% of DAU are active in a 10-min window. 1M users.\n6. Refresh: Let's say 20% of *active* users refresh *within* 1 minute of the update. 1,000,000 * 0.20 = 200,000 users.\n7. Peak QPS: 200,000 requests / 60 seconds ≈ 3,333 RPS.\nThis service needs to handle a read-spike of **~3,500 QPS** every 10 minutes.", "rejected": "10 million users. It updates every 10 minutes. 10,000,000 / (10 * 60) = 16,666 RPS. (Assumes all 10M users fetch it every 10 mins)."}
{"prompt": "How much data is transferred if 1 million users download a 500KB JSON payload?", "chosen": "1. Users: 1,000,000\n2. Payload: 500 KB\n3. Calculation: 1,000,000 * 500 KB = 500,000,000 KB.\n4. Conversion: 500,000,000 KB = 500,000 MB = 500 GB.\nThis would be **500 GB** of data transfer.", "rejected": "1 million * 500KB = 500,000 KB. That's 500MB. (Simple math error)."}
{"prompt": "Estimate the storage for 10 million users, each with 5 'favorites' (an array of 8-byte IDs).", "chosen": "1. Users: 10,000,000\n2. Favorites/user: 5\n3. ID size: 8 bytes (BIGINT)\n4. Data per user: 5 * 8 bytes = 40 bytes.\n5. Overhead: Storing this in a JSONB or array field adds overhead. Let's say 20 bytes of overhead. Total = 60 bytes/user.\n6. Calculation: 10,000,000 users * 60 bytes/user = 600,000,000 bytes.\n7. Conversion: 600,000,000 bytes = 600 MB.\nThis data would take **around 600 MB**.", "rejected": "Each favorite is an object, 1KB each. 10M users * 5 favorites * 1KB = 50,000,000 KB = 50GB."}
{"prompt": "How many t3.micro instances could you run for $100/month (on-demand, us-east-1)?", "chosen": "1. Monthly budget: $100.\n2. Instance price: t3.micro on-demand is ~$0.0104 per hour.\n3. Hours/month: ~720.\n4. Cost per instance/month: $0.0104 * 720 = $7.488.\n5. Calculation: $100 / $7.488/instance ≈ 13.35 instances.\nYou could run **13 instances** 24/7.", "rejected": "A t3.micro is tiny, maybe $1/month. You can run 100 instances."}
{"prompt": "How long to re-index 50GB of documents in Elasticsearch if the node can index at 2,000 docs/sec and avg doc size is 5KB?", "chosen": "1. Total data: 50 GB = 50,000,000 KB.\n2. Doc size: 5 KB.\n3. Total documents: 50,000,000 KB / 5 KB/doc = 10,000,000 documents.\n4. Indexing rate: 2,000 docs/sec.\n5. Total time: 10,000,000 docs / 2,000 docs/sec = 5,000 seconds.\n6. Conversion: 5,000 sec / 3600 sec/hr ≈ 1.38 hours.\nIt would take **around 1.4 hours**.", "rejected": "50GB is 50,000 MB. Indexing is slow, maybe 1 MB/sec. 50,000 / 1 = 50,000 seconds. 13.8 hours."}
{"prompt": "Estimate the daily cost of running a 1TB data warehouse (like Redshift or BigQuery) that scans 20TB of data per day.", "chosen": "1. This depends on the model. \n2. Redshift (provisioned): A ra3.xlplus node is ~$1/hr, has 32TB. ~$24/day for compute. Storage is ~$24/TB-mo. 1TB = $0.80/day. Total = **~$25/day** (plus query concurrency).\n3. BigQuery (on-demand): Storage: 1TB * $0.02/GB-mo = 1000 * 0.02 = $20/mo = ~$0.67/day. Querying: 20TB * $6.25/TB = $125/day. Total = **~$126/day**.\n4. Conclusion: The cost varies hugely. For a scan-heavy workload, BigQuery on-demand is more expensive. Let's provide the BigQuery estimate: **~ $125 / day**.", "rejected": "20TB scan. That's $1 per TB. $20/day. (Uses arbitrary pricing)."}
{"prompt": "How much storage for 1 million 3-minute audio files (at 128 kbps)?", "chosen": "1. Files: 1,000,000\n2. Length: 3 minutes = 180 seconds.\n3. Bitrate: 128 kbps (kilobits per second). \n4. Size per file (bits): 180 sec * 128,000 bits/sec = 23,040,000 bits.\n5. Size per file (bytes): 23,040,000 / 8 bits/byte = 2,880,000 bytes ≈ 2.88 MB.\n6. Total storage: 1,000,000 files * 2.88 MB/file = 2,880,000 MB = 2,880 GB = 2.88 TB.\nWe'd need **around 2.9 TB**.", "rejected": "1 million files. Each is 3 minutes. An mp3 is 1MB per minute. 1M * 3 * 1MB = 3,000,000 MB = 3TB. (This is a good estimate, but the 'rejected' one should be wrong). Let's make it wrong: 128 kbps = 128 KB/s. 180s * 128KB/s = 23,040 KB = 23MB. 1M * 23MB = 23TB."}
{"prompt": "Estimate the storage for 1 year of 1-second-resolution metrics for 10,000 devices, each sending 5 metrics.", "chosen": "1. Devices: 10,000\n2. Metrics/device: 5\n3. Total metrics/sec: 10,000 * 5 = 50,000 metrics/sec.\n4. Datapoint size: Let's say 16 bytes (timestamp + value).\n5. Data/sec: 50,000 * 16 bytes = 800,000 bytes/sec = 0.8 MB/s.\n6. Data/day: 0.8 MB/s * 86,400 s/day = 69,120 MB/day ≈ 69 GB/day.\n7. Data/year: 69 GB/day * 365 days/year ≈ 25,185 GB ≈ 25 TB.\n8. Compression: Time-series DBs compress this well, maybe 10x.\n9. Compressed: 25 TB / 10 = 2.5 TB.\nWe'd need **~2.5 TB** of storage.", "rejected": "10,000 devices * 5 metrics * 16 bytes = 800,000 bytes. That's 800KB. It's tiny. (Ignores 'per second' and 'per year')."}
{"prompt": "A cron job runs every 10 minutes and takes 2 minutes to run. How many can run in parallel on one server?", "chosen": "1. Job interval: 10 minutes.\n2. Job duration: 2 minutes.\n3. This is a concurrency question. A new job starts every 10 minutes. The old job finishes after 2 minutes.\n4. Timeline:\n   - 00:00: Job 1 starts\n   - 00:02: Job 1 finishes\n   - 00:10: Job 2 starts\n   - 00:12: Job 2 finishes\n5. Conclusion: At no point are two jobs running at the same time. You only need 1 worker or process. The server can run **only 1** of *this specific job* in 'parallel' because they are sequential by definition.", "rejected": "10 min interval / 2 min duration = 5. You can run 5 jobs in parallel."}
{"prompt": "How many 1KB 'todo' items can be stored in 1GB of memory?", "chosen": "1. Memory: 1 GB = 1,000 MB = 1,000,000 KB.\n2. Item size: 1 KB.\n3. Calculation: 1,000,000 KB / 1 KB/item = 1,000,000 items.\n4. Overhead: Storing this in a data structure (like a hash map or list) adds overhead (pointers, object headers). Let's say 30% overhead.\n5. Realistic: 1,000,000 * 0.70 = 700,000.\nWe can store **around 700,000** items.", "rejected": "1GB = 1,024 MB = 1,048,576 KB. 1,048,576 / 1 = 1,048,576. Exactly 1,048,576 items. (Ignores any overhead)."}
{"prompt": "Estimate the monthly data transfer cost for a 1,000-user app, where each user syncs 10MB of data daily.", "chosen": "1. Users: 1,000\n2. Data/user/day: 10 MB\n3. Total data/day: 1,000 * 10 MB = 10,000 MB = 10 GB.\n4. Total data/month: 10 GB/day * 30 days/month = 300 GB.\n5. Cost (AWS egress): Let's assume it's all egress. The first 100GB/mo is free.\n6. Billable data: 300 GB - 100 GB = 200 GB.\n7. Cost: 200 GB * $0.09/GB (standard rate) = $18.\nThe cost would be **around $18 per month**.", "rejected": "1000 users * 10MB = 10GB/day. 30 days = 300GB. 300 * $0.09 = $27. (This ignores the 100GB free tier)."}
{"prompt": "How long to brute-force a 6-character alphanumeric (case-sensitive) password at 1,000 guesses/sec?", "chosen": "1. Character set: 26 lowercase + 26 uppercase + 10 digits = 62 characters.\n2. Possibilities: 62^6 (62 * 62 * 62 * 62 * 62 * 62).\n3. Calculation: 62^2 ≈ 3,800. 62^3 ≈ 240,000. 62^6 = (62^3)^2 ≈ (240,000)^2 = 57,600,000,000. Let's use 57 billion.\n4. Guess rate: 1,000 guesses/sec.\n5. Time for 100%: 57,000,000,000 / 1,000 = 57,000,000 seconds.\n6. Average time (50%): 28,500,000 seconds.\n7. Conversion: 28,500,000 / 3600 sec/hr = 7,916 hours. 7,916 / 24 hr/day = 329 days.\nIt would take **on average, 329 days**.", "rejected": "6 characters. 1000 guesses/sec. 6 * 1000 = 6000. It will take a few hours."}
{"prompt": "How many servers to store 1PB of data with 3x replication, using 10TB drives per server?", "chosen": "1. Data: 1 PB.\n2. Replication: 3x. Total data to store = 1 PB * 3 = 3 PB.\n3. Drive size: 10 TB.\n4. Drives needed: 3 PB = 3,000 TB. 3,000 TB / 10 TB/drive = 300 drives.\n5. Drives per server: A server might hold 12 drives. \n6. Servers needed: 300 drives / 12 drives/server = 25 servers.\nWe'd need **25 servers**.", "rejected": "1PB / 10TB = 100 servers. (Ignores replication)."}
{"prompt": "Estimate the API bandwidth for 50,000 users, each sending a 1KB 'heartbeat' every 30 seconds.", "chosen": "1. Users: 50,000\n2. Payload: 1 KB\n3. Frequency: Every 30 seconds.\n4. Requests per user/sec: 1 / 30s = 0.033 RPS.\n5. Total RPS: 50,000 users * (1/30) RPS = 1,666.6 RPS.\n6. Bandwidth (ingress): 1,666.6 RPS * 1 KB/req = 1,666.6 KB/s ≈ 1.6 MB/s.\n7. Conversion to bits: 1.6 MB/s * 8 bits/byte = 12.8 Mbit/s.\nWe'd need **~13 Mbit/s** of ingress bandwidth.", "rejected": "50,000 users * 1KB = 50,000 KB = 50MB. We need 50MB/s. (Ignores frequency)."}
{"prompt": "How many 500-byte user 'events' can be stored in 10TB?", "chosen": "1. Storage: 10 TB = 10,000 GB = 10,000,000 MB = 10,000,000,000 KB.\n2. Event size: 500 bytes = 0.5 KB.\n3. Calculation: 10,000,000,000 KB / 0.5 KB/event = 20,000,000,000 events.\nWe can store **20 billion events**.", "rejected": "10TB / 500 bytes = 20,000. You can store 20,000 events. (Fails at unit conversion)."}
{"prompt": "Estimate the size of a 100M row table with 2 BIGINT columns and 1 UUID column.", "chosen": "1. Rows: 100,000,000\n2. Column sizes:\n   - BIGINT: 8 bytes\n   - BIGINT: 8 bytes\n   - UUID: 16 bytes (if stored as binary)\n3. Row overhead: ~24 bytes (PostgreSQL/MySQL).\n4. Total per row: 8 + 8 + 16 + 24 = 56 bytes.\n5. Data size: 100,000,000 * 56 bytes = 5,600,000,000 bytes = 5.6 GB.\n6. Fill factor (90%): 5.6 GB / 0.9 ≈ 6.2 GB.\nThe table data would be **around 6.2 GB**.", "rejected": "A UUID is 36 characters. 8 + 8 + 36 = 52 bytes. 100M * 52 = 5.2GB. (This is close, but assumes text storage for UUID, and ignores all row/page overhead)."}
{"prompt": "How long to transfer 50GB over a 100 Mbps (megabit) connection?", "chosen": "1. Data: 50 GB.\n2. Speed: 100 Mbps.\n3. Units: 50 GB = 50 * 1000 MB = 50,000 MB. 50 GB = 400 Gbit.\n   100 Mbps = 100 / 8 = 12.5 MBytes/s.\n4. Calculation (Bytes): 50,000 MB / 12.5 MB/s = 4,000 seconds.\n   Calculation (bits): 400 Gbit = 400,000 Mbit. 400,000 Mbit / 100 Mbps = 4,000 seconds.\n5. Conversion: 4,000 sec / 60 min ≈ 66.6 minutes.\nIt would take **about 67 minutes**.", "rejected": "50GB = 50,000 MB. Speed is 100 Mbps. 50,000 / 100 = 500 seconds. (Confuses MB and Mb)."}
{"prompt": "How many 20KB JSON documents can be stored in 1TB?", "chosen": "1. Storage: 1 TB = 1,000,000,000 KB.\n2. Doc size: 20 KB.\n3. Calculation: 1,000,000,000 KB / 20 KB/doc = 50,000,000 documents.\n4. Overhead: Storing in a doc database (like Mongo) adds overhead (BSON, indexes). Let's say 20% overhead.\n5. Realistic: 50,000,000 * 0.8 = 40,000,000.\nWe can store **around 40 million documents**.", "rejected": "1TB = 1,000 GB. 1GB = 1,000 MB. 1TB = 1,000,000 KB. 1,000,000 / 20 = 50,000 documents. (Math error)."}
{"prompt": "Estimate the monthly cost of 100TB of EBS gp3 storage (in us-east-1).", "chosen": "1. Storage: 100 TB = 100,000 GB.\n2. Price: gp3 storage is $0.08 per GB-month.\n3. Calculation: 100,000 GB * $0.08/GB = $8,000.\n4. Note: This assumes baseline 3,000 IOPS, which is included. Provisioning more IOPS or throughput would cost extra.\nThe baseline cost is **$8,000 per month**.", "rejected": "EBS is cheap, $0.01/GB. 100,000 * 0.01 = $1,000/month."}
{"prompt": "A service runs at 2,000 RPS. How many requests per day?", "chosen": "1. RPS: 2,000\n2. Seconds per day: 24 * 60 * 60 = 86,400 seconds.\n3. Calculation: 2,000 req/sec * 86,400 sec/day = 172,800,000 req/day.\nThe service handles **~173 million requests per day**.", "rejected": "2,000 RPS * 60 sec = 120,000 requests per minute. 120,000 * 60 min = 7,200,000 requests per hour. (Stops here, doesn't answer 'per day')."}
{"prompt": "How many 4KB 'read' operations can a single spinning disk (HDD) do per second (IOPS)?", "chosen": "1. This is a function of seek time and rotational latency.\n2. Seek time: ~5-10 ms (average).\n3. Rotational latency (7200 RPM): (60 sec / 7200) / 2 (avg) ≈ 4.1 ms.\n4. Total time per I/O: 8ms (seek) + 4.1ms (rotate) ≈ 12.1 ms.\n5. IOPS: 1 second / 0.0121 seconds/op ≈ 82.6 IOPS.\nFor random 4K reads, a 7200 RPM HDD can do **around 80-100 IOPS**.", "rejected": "A disk is fast. It can do thousands. At least 5,000 IOPS."}
{"prompt": "How many 4KB 'read' operations can a single SSD do per second (IOPS)?", "chosen": "1. This is a function of the controller and NAND flash speed.\n2. Latency: SSDs don't have seek time. Latency is very low, e.g., 0.1 ms (100 microseconds).\n3. Sequential: 1 sec / 0.0001 sec/op = 10,000 IOPS.\n4. Reality: It's limited by the internal parallelism. A good commodity SSD can do **50,000 to 100,000** random read IOPS. A high-end datacenter NVMe drive can do 500,000+.\nLet's estimate **70,000 IOPS** for a good drive.", "rejected": "An SSD is 10x faster than an HDD. An HDD does 100 IOPS. So an SSD does 10 * 100 = 1,000 IOPS."}
{"prompt": "Estimate the storage for 1 billion 100-byte events.", "chosen": "1. Events: 1,000,000,000 ($10^9$)\n2. Event size: 100 bytes\n3. Calculation: $10^9$ * 100 bytes = $100 \times 10^9$ bytes = 100,000,000,000 bytes.\n4. Conversion: 100,000,000,000 bytes = 100,000 MB = 100 GB.\nWe'd need **100 GB** for the raw data.", "rejected": "1 billion * 100 = 100 billion. That's 100TB."}
{"prompt": "A 100GB database is snapshotted every hour. How much *additional* storage is used per day, assuming 1% change per hour?", "chosen": "1. DB size: 100 GB.\n2. Change rate: 1% per hour = 100 GB * 0.01 = 1 GB per hour.\n3. Snapshots: Modern snapshots are incremental. The *first* snapshot is 100GB. \n4. Incremental: Each subsequent snapshot only stores the *changes* (the 1GB).\n5. Total snapshots/day: 24.\n6. First: 100 GB.\n7. Incrementals: 23 * 1 GB = 23 GB.\n8. Total at end of day: 100 GB (base) + 23 GB (deltas) = 123 GB. \n9. *Additional* storage: The deltas. 24 hours * 1 GB/hr = 24GB. (The first snapshot replaces the previous base).\nLet's re-think: A snapshot freezes the 100GB. The 1GB/hr change is new data. After 24 hours, we have 24 snapshots. We have the original 100GB + 24GB of changed blocks. The *total* storage used by the snapshot chain is 124GB. The *additional* storage is **24 GB**.", "rejected": "100GB * 24 times/day = 2,400 GB = 2.4TB. (Assumes full, non-incremental copies)."}
{"prompt": "How many 1M 'like' events (user_id, post_id) can be processed per second by a 4-core server, if processing 1 event takes 1ms of CPU time?", "chosen": "1. This is CPU-bound. \n2. Time per event: 1ms = 0.001 seconds.\n3. Events per core/sec: 1 second / 0.001 sec/event = 1,000 events/sec.\n4. Total events/sec: 1,000 events/sec/core * 4 cores = 4,000 events/sec.\nThe server can process **4,000 'like' events per second**.", "rejected": "1 million events. 1ms each. 1,000,000 * 0.001 = 1,000 seconds. It takes 1,000 seconds. (Answers 'how long', not 'how many per sec')."}
{"prompt": "Estimate the monthly egress cost for an app serving 10M 10KB images per day from S3.", "chosen": "1. Images/day: 10,000,000\n2. Size: 10 KB\n3. Data/day: 10,000,000 * 10 KB = 100,000,000 KB = 100,000 MB = 100 GB.\n4. Data/month: 100 GB/day * 30 days/month = 3,000 GB = 3 TB.\n5. AWS Egress Cost (assume to internet):\n   - First 100 GB: Free\n   - Next 2,900 GB: $0.09/GB (This is in the 0-10TB tier)\n6. Calculation: 2,900 GB * $0.09/GB = $261.\nThe cost would be **~$261 per month**.", "rejected": "3,000 GB * $0.09/GB = $270. (Close, but ignores the free tier)."}
{"prompt": "How long would it take to create 1 billion SHA-256 hashes on a single CPU core that can do 10 million hashes/sec?", "chosen": "1. Total hashes: 1,000,000,000\n2. Rate: 10,000,000 hashes/sec.\n3. Calculation: 1,000,000,000 / 10,000,000 = 100 seconds.\nIt would take **100 seconds**.", "rejected": "1 billion is a huge number. This will take hours. 1,000,000,000 / 10,000,000 = 100. 100 * 60 = 6,000 seconds. (Math error)."}
{"prompt": "Estimate the RAM needed for 5,000 concurrent WebSocket connections, if each connection uses 100KB of buffer/state.", "chosen": "1. Connections: 5,000\n2. RAM/connection: 100 KB\n3. Calculation: 5,000 * 100 KB = 500,000 KB.\n4. Conversion: 500,000 KB = 500 MB.\n5. Overhead: The OS and server process (e.g., Nginx, Node.js) also need RAM. Let's add 500MB for the base system.\n6. Total: 500MB (connections) + 500MB (system) = 1 GB.\nWe'd need a server with **at least 1 GB of RAM**.", "rejected": "5,000 connections * 100KB = 500,000 KB = 500GB. You need a 500GB RAM server."}
{"prompt": "How many 1KB read ops can a 10GbE (Gigabit Ethernet) NIC handle per second?", "chosen": "1. NIC speed: 10 Gbit/s.\n2. Data size: 1 KB = 8 Kbit.\n3. Theoretical max (bandwidth): 10 Gbit/s / 8 Kbit/op = 10,000,000 Kbit/s / 8 Kbit/op = 1,250,000 ops/sec.\n4. Packet limit: Network ops are also limited by packets/sec. A 1KB op fits in one standard 1500-byte packet. A 10GbE NIC can often handle 1-2 million packets per second (PPS). \n5. Conclusion: The limits are similar. The NIC can theoretically handle **~1.25 million 1KB reads/sec**.", "rejected": "10 Gbit/s = 10,000 Mbit/s. 1KB = 8 Kbit. 10,000 / 8 = 1,250 ops/sec. (Confuses Gbit and Mbit)."}
{"prompt": "Estimate the monthly cost of a single AWS m5.large instance (reserved, 1-year, no upfront).", "chosen": "1. Instance: m5.large\n2. On-demand price: ~$0.096/hour.\n3. Reserved price (1-yr, no upfront): ~$0.06/hour (this is a ~37% discount).\n4. Hours/month: 720.\n5. Calculation: 720 hours * $0.06/hour = $43.20.\nThe reserved instance cost would be **~$43 per month**.", "rejected": "On-demand is $0.096/hr * 720 = $69.12. Reserved is half off. $34.56. (Discount is not 50%)."}
{"prompt": "How many 200-byte log lines can be stored in 5GB?", "chosen": "1. Storage: 5 GB = 5,000,000,000 bytes.\n2. Log line size: 200 bytes.\n3. Calculation: 5,000,000,000 bytes / 200 bytes/line = 25,000,000 lines.\nWe can store **25 million log lines**.", "rejected": "5GB = 5,000 MB = 5,000,000 KB. 200 bytes = 0.2KB. 5,000,000 / 0.2 = 25,000,000. (This is correct, but let's make the rejected one wrong). 5GB = 5,000,000,000 bytes. 200 bytes. 5,000,000,000 / 200 = 2,500,000. (Math error)."}
{"prompt": "Estimate the size of a user table for 2.5 million users, 500 bytes per user row.", "chosen": "1. Users: 2,500,000\n2. Row size: 500 bytes.\n3. Data size: 2,500,000 * 500 bytes = 1,250,000,000 bytes.\n4. Conversion: 1,250,000,000 bytes = 1,250 MB = 1.25 GB.\n5. Overhead: Add 20% for page headers/fill factor. 1.25 GB * 1.2 = 1.5 GB.\nThe table would be **around 1.5 GB** (plus indexes).", "rejected": "2.5 million * 500 = 1,250 million. That's 1.25 TB."}
{"prompt": "How many 50KB JSON objects can be stored in 10GB?", "chosen": "1. Storage: 10 GB = 10,000 MB = 10,000,000 KB.\n2. Object size: 50 KB.\n3. Calculation: 10,000,000 KB / 50 KB/object = 200,000 objects.\nWe can store **200,000 objects** (before DB overhead).", "rejected": "10GB = 10,000 MB. 50KB = 0.05MB. 10,000 / 0.05 = 200,000. (This is also correct. Let's make rejected wrong). 10GB = 10,000,000 KB. 10,000,000 / 50 = 2,000,000. (Math error)."}
{"prompt": "Estimate the daily log storage for 1,000 servers, each generating 100 log lines/sec, with 200 bytes/line.", "chosen": "1. Servers: 1,000\n2. Lines/sec/server: 100\n3. Total lines/sec: 1,000 * 100 = 100,000 lines/sec.\n4. Line size: 200 bytes.\n5. Data/sec: 100,000 * 200 bytes = 20,000,000 bytes/sec = 20 MB/s.\n6. Data/day: 20 MB/s * 86,400 sec/day = 1,728,000 MB/day = 1.728 TB/day.\nWe'd generate **~1.7 TB of logs per day**.", "rejected": "1000 servers * 100 lines/sec * 200 bytes = 20MB/s. That's 20MB * 60 * 60 = 72GB/hour. (Stops at per-hour)."}
{"prompt": "A dev team of 10 costs $150k/year each. What is the 'cost per hour' of the team?", "chosen": "1. Team cost/year: 10 * $150,000 = $1,500,000/year.\n2. Work hours/year: 40 hours/week * 50 weeks/year (2 weeks vacation) = 2,000 hours/year.\n3. Cost per hour: $1,500,000 / 2,000 hours = $750 per hour.\n4. Loaded cost: Salary is not total cost. Add 30-50% for benefits, tax, office, etc. $750 * 1.4 = $1,050/hour.\nThe 'fully loaded' cost of the team is **~$1,000 per hour**.", "rejected": "10 * 150k = 1.5M. Per year. Per hour? $150,000 / (365*24) = $17/hour. 10 * 17 = $170/hour. (Uses 24/7/365, not work hours)."}
{"prompt": "Estimate the monthly cost for 1PB of S3 Standard storage.", "chosen": "1. Storage: 1 PB = 1,000 TB = 1,000,000 GB.\n2. Pricing (tiered): \n   - 50TB ($0.023) = $1,150\n   - 450TB ($0.022) = $9,900\n   - 500TB ($0.021) = $10,500\n3. Total: $1,150 + $9,900 + $10,500 = $21,550.\n4. Blended rate check: $21,550 / 1,000,000 GB = $0.02155/GB. This looks right.\nThe cost is **~$21,550 per month**.", "rejected": "1 PB = 1,000,000 GB. Price is $0.023/GB. 1,000,000 * 0.023 = $23,000. (Ignores tiered pricing)."}
{"prompt": "How many 8-byte numbers can you fit in 16GB of RAM?", "chosen": "1. RAM: 16 GB = 16,000,000,000 bytes (using 10^9).\n2. Number size: 8 bytes.\n3. Calculation: 16,000,000,000 / 8 = 2,000,000,000.\n4. Using binary GiB: 16 GiB = 16 * 1024^3 ≈ 17.17 billion bytes. 17.17B / 8 = 2.14B.\nLet's stick to the 10^9 convention: **2 billion numbers**.", "rejected": "16GB. 8 bytes. 16 / 8 = 2. You can fit 2. (Massive unit error)."}
{"prompt": "Estimate the API bandwidth for 1M DAU, 100 req/user/day, 2KB avg payload.", "chosen": "1. DAU: 1,000,000\n2. Req/user: 100\n3. Total req/day: 1M * 100 = 100,000,000 req/day.\n4. Avg payload: 2 KB.\n5. Total data/day: 100,000,000 * 2 KB = 200,000,000 KB = 200,000 MB = 200 GB/day.\n6. Avg bandwidth (bits): 200 GB = 1600 Gbit. 1600 Gbit / 86,400 sec ≈ 18.5 Gbit/s. This is wrong. 1,600,000 Mbit / 86,400s = 18.5 Mbit/s.\n7. Peak: Let's say 10x peak. 18.5 * 10 = 185 Mbit/s.\nWe need to handle **~185 Mbit/s** peak bandwidth.", "rejected": "1M users * 100 req * 2KB = 200,000,000 KB = 200TB/day. (Math error)."}
{"prompt": "How many 1KB 'user settings' objects can be stored in a 5GB Redis cluster?", "chosen": "1. Storage: 5 GB = 5,000,000 KB.\n2. Object size: 1 KB (value).\n3. Key size: Redis also stores the key. Let's say user_id_settings, avg 20 bytes.\n4. Overhead: Redis has object/memory overhead. Let's say 50 bytes per key.\n5. Total per item: 1KB (value) + 20b (key) + 50b (overhead) ≈ 1.07 KB.\n6. Calculation: 5,000,000 KB / 1.07 KB/item ≈ 4,670,000.\nWe can store **around 4.6 million** objects.", "rejected": "5GB = 5,000,000 KB. 1KB per object. 5,000,000 / 1 = 5 million. Exactly. (Ignores keys and all overhead)."}
{"prompt": "How long to run a full table scan on a 1TB table on HDDs, if the server has 100 MB/s disk I/O?", "chosen": "1. Data size: 1 TB = 1,000,000 MB.\n2. I/O speed: 100 MB/s.\n3. Calculation: 1,000,000 MB / 100 MB/s = 10,000 seconds.\n4. Conversion: 10,000 sec / 3600 sec/hr ≈ 2.77 hours.\nIt would take **about 2.8 hours**.", "rejected": "1TB = 1,000 GB. 100MB/s = 0.1GB/s. 1,000 / 0.1 = 10,000. It takes 10,000 hours."}
{"prompt": "How long to scan 1TB on an NVMe SSD with 3,000 MB/s read speed?", "chosen": "1. Data size: 1 TB = 1,000,000 MB.\n2. I/O speed: 3,000 MB/s.\n3. Calculation: 1,000,000 MB / 3,000 MB/s = 333.3 seconds.\n4. Conversion: 333.3 sec / 60 sec/min ≈ 5.5 minutes.\nIt would take **about 5.5 minutes**.", "rejected": "SSDs are fast. It will take 1 second."}
{"prompt": "Estimate the cost of a 'git commit' hook that runs a 30-second CI test, for a team of 50 engineers committing 5 times/day.", "chosen": "1. Engineers: 50\n2. Commits/day: 5\n3. Total commits/day: 50 * 5 = 250 commits/day.\n4. Test duration: 30 seconds.\n5. Total CI time/day: 250 * 30 sec = 7,500 seconds.\n6. Conversion: 7,500 / 3600 = 2.08 hours/day.\n7. Cost: Let's use a CI provider cost, e.g., $0.008/minute. 2.08 hours = 125 minutes. 125 * $0.008 = $1/day.\n8. Monthly: $1/day * 22 work days/month = $22/month.\nThis is very cheap, **around $20-30/month**.", "rejected": "50 engineers * 5 commits = 250. 250 * 30 seconds = 7500. This is a huge number. This will cost $10,000/month in CI bills."}
{"prompt": "How many 5-line functions can a 100k line-of-code (LOC) codebase have?", "chosen": "1. LOC: 100,000\n2. Function size: 5 lines.\n3. Calculation: 100,000 / 5 = 20,000.\n4. Overhead: Not all code is in functions (imports, comments, classes, blank lines). Let's say 50% is 'other'.\n5. Realistic LOC: 100,000 * 0.50 = 50,000 LOC in functions.\n6. Overhead 2: Functions also have a signature/definition line. So 5 lines of code = 6 lines total. 50,000 / 6 ≈ 8,333.\nI'd estimate **around 8,000 functions**.", "rejected": "100,000 / 5 = 20,000. Exactly 20,000 functions."}
{"prompt": "Estimate the storage for 10,000 Git repositories, averaging 200MB each.", "chosen": "1. Repos: 10,000\n2. Avg size: 200 MB\n3. Calculation: 10,000 * 200 MB = 2,000,000 MB.\n4. Conversion: 2,000,000 MB = 2,000 GB = 2 TB.\nWe'd need **2 TB** of storage.", "rejected": "10,000 * 200 = 2,000,000. That's 2 million TB."}
{"prompt": "How many 1KB 'read' QPS can a 1Gbit/s network link support?", "chosen": "1. Link speed: 1 Gbit/s.\n2. Read size: 1 KB = 8 Kbit.\n3. Calculation: 1 Gbit/s / 8 Kbit/op = 1,000,000 Kbit/s / 8 Kbit/op = 125,000 QPS.\n4. Overhead: This assumes 100% data. Network protocols (TCP/IP) add overhead, maybe 10%.\n5. Realistic: 125,000 * 0.9 = 112,500.\nIt can support **around 110,000-125,000 QPS**.", "rejected": "1 Gbit/s = 1,000,000,000 bits. 1KB = 1000 bytes. 1,000,000,000 / 1000 = 1,000,000 QPS."}
{"prompt": "A 1M-user app adds 10KB of data per user per year. How much new storage per month?", "chosen": "1. Users: 1,000,000\n2. Data/user/year: 10 KB\n3. Total data/year: 1,000,000 * 10 KB = 10,000,000 KB = 10 GB.\n4. Data/month: 10 GB / 12 months ≈ 0.833 GB.\n5. Conversion: 0.833 GB = 833 MB.\nWe'd add **~833 MB** of new storage per month.", "rejected": "1M users * 10KB = 10GB. We need 10GB per month."}
{"prompt": "How many 100-byte user records can you store in a 4MB 'page' in a database?", "chosen": "1. Page size: 4 MB = 4,000,000 bytes.\n2. Record size: 100 bytes.\n3. Calculation: 4,000,000 / 100 = 40,000 records.\n4. Overhead: A page has a header. Records have headers. Let's say 10% overhead total.\n5. Realistic: 40,000 * 0.9 = 36,000.\nThis seems high. Let's re-calc: 4MB = 4096 bytes. 4096 / 100 = 40. No, 4MB = 4,194,304 bytes. 4,194,304 / 100 = 41,943. \nWait, a DB page is 4KB or 8KB. Let's assume 8KB. 8192 bytes. 8192 / 100 ≈ 81. Let's re-read. '4MB page' - maybe a 'block' or 'extent'. \nLet's assume the question meant 4KB. 4096 bytes / 100 ≈ 40 (with overhead). \nIf it *really* means 4MB: 4,194,304 bytes. Let's say 110 bytes with overhead. 4,194,304 / 110 ≈ 38,130.\nLet's assume the question meant 4KB, as 4MB is not a page size. 4KB = 4096 bytes. (4096 - 96 byte header) / (100 byte data + 24 byte header) = 4000 / 124 ≈ 32.\nI'll answer assuming 4KB: **About 32 records**.", "rejected": "4MB = 4,000,000 bytes. 4,000,000 / 100 = 40,000. Exactly 40,000."}
{"prompt": "How many 16-core servers for a 10,000 RPS workload, if 1 core = 100 RPS?", "chosen": "1. Total RPS: 10,000\n2. Server cores: 16\n3. RPS per core: 100\n4. RPS per server: 16 cores * 100 RPS/core = 1,600 RPS.\n5. Servers needed: 10,000 RPS / 1,600 RPS/server = 6.25 servers.\n6. Redundancy: We need to round up (7) and add N+1. 7 + 1 = 8.\nWe'd need **8 servers**.", "rejected": "10,000 RPS / 100 RPS = 100. We need 100 cores. 100 / 16 = 6.25. So 6 servers. (Doesn't round up, no redundancy)."}
{"prompt": "Estimate the monthly cost of a 500GB 'hot' SSD (io1) EBS volume with 10,000 IOPS (us-east-1).", "chosen": "1. Storage: 500 GB * $0.125/GB-mo = $62.50.\n2. IOPS: io1 cost is $0.065 per provisioned IOPS-month.\n   10,000 IOPS * $0.065/IOPS-mo = $650.\n3. Total: $62.50 (storage) + $650 (IOPS) = $712.50.\nThe cost is **~$712.50 per month**.", "rejected": "500GB * $0.125 = $62.50. (Ignores the cost of IOPS, which is the main driver)."}
{"prompt": "How long to download 100GB over a 50 Mbps (megabit) home internet connection?", "chosen": "1. Data: 100 GB.\n2. Speed: 50 Mbps.\n3. Units: 100 GB = 800 Gbit = 800,000 Mbit.\n   50 Mbps = 6.25 MBytes/s.\n4. Calculation (bits): 800,000 Mbit / 50 Mbps = 16,000 seconds.\n5. Conversion: 16,000 sec / 3600 sec/hr = 4.44 hours.\nIt would take **about 4.4 hours**.", "rejected": "100GB = 100,000 MB. 50 Mbps. 100,000 / 50 = 2,000 seconds. (Confuses MB and Mb)."}
{"prompt": "Estimate the cost to store 10 million 1KB JSON files in S3 and read them all once per month.", "chosen": "1. Files: 10,000,000\n2. Size: 1 KB\n3. Storage: 10,000,000 * 1 KB = 10,000,000 KB = 10 GB.\n4. Storage cost: 10 GB * $0.023/GB-mo = $0.23.\n5. Read requests: 10,000,000 GET requests. Cost is $0.0004 per 1,000. 10,000,000 / 1000 * $0.0004 = 10,000 * $0.0004 = $4.00.\n6. Data egress: 10GB read. First 100GB is free. So $0.\n7. Total: $0.23 (storage) + $4.00 (reads) = $4.23.\nThe cost would be **~$4.23 per month**.", "rejected": "Storage is $0.23. Reads are free. It costs 23 cents."}
{"prompt": "How many 2KB 'user session' objects can a 64GB server hold in RAM?", "chosen": "1. RAM: 64 GB = 64,000,000 KB.\n2. Object size: 2 KB.\n3. Calculation: 64,000,000 KB / 2 KB/obj = 32,000,000 objects.\n4. Overhead: OS (~2GB), Redis/cache overhead (~25%). Let's say 60GB is usable. 60,000,000 KB. Let's say 2.5KB total per obj. 60,000,000 / 2.5 = 24,000,000.\nWe can hold **around 24 million** sessions.", "rejected": "64 GB / 2 KB = 32. You can hold 32 sessions. (Unit error)."}
{"prompt": "Estimate the size of a 1 billion row, 2-column (INT, INT) table.", "chosen": "1. Rows: 1,000,000,000\n2. Columns: INT (4 bytes) + INT (4 bytes) = 8 bytes.\n3. Row overhead: ~24 bytes.\n4. Total per row: 8 + 24 = 32 bytes.\n5. Data size: 1,000,000,000 * 32 bytes = 32,000,000,000 bytes = 32 GB.\n6. Fill factor (90%): 32 / 0.9 ≈ 35.5 GB.\nThe table data would be **~36 GB**.", "rejected": "1B rows * 2 columns = 2B. 2 billion * 4 bytes = 8GB. (Ignores row overhead)."}
{"prompt": "How many 1KB 'read' QPS can a 100Mbit/s network link support?", "chosen": "1. Link: 100 Mbit/s.\n2. Read size: 1 KB = 8 Kbit.\n3. Calculation: 100 Mbit/s / 8 Kbit/op = 100,000 Kbit/s / 8 Kbit/op = 12,500 QPS.\n4. Overhead (10%): 12,500 * 0.9 = 11,250.\nIt can support **around 11,000-12,500 QPS**.", "rejected": "100 Mbit/s = 100 MB/s. 100 / 0.001 = 100,000 QPS. (Confuses Mb and MB)."}
{"prompt": "Estimate the storage for 500,000 users, each uploading a 50KB avatar.", "chosen": "1. Users: 500,000\n2. Size: 50 KB\n3. Calculation: 500,000 * 50 KB = 25,000,000 KB.\n4. Conversion: 25,000,000 KB = 25,000 MB = 25 GB.\nWe'd need **25 GB** of storage.", "rejected": "500,000 * 50 = 2,500,000. That's 2.5GB. (Math error)."}
{"prompt": "How long to iterate 1 billion items in Python on a single core, if one iteration is 100ns (nanoseconds)?", "chosen": "1. Items: $10^9$\n2. Time/item: 100 ns = $100 \times 10^{-9}$ seconds = $10^{-7}$ seconds.\n3. Calculation: $10^9$ items * $10^{-7}$ sec/item = $10^2$ seconds = 100 seconds.\nIt would take **100 seconds**.", "rejected": "1 billion * 100 = 100 billion. 100 billion nanoseconds. That's 100 * 10^9 * 10^-9 = 100. Wait, that's right. Rejected: 1 billion * 100 = 100,000,000,000. That's a huge number, will take years."}
{"prompt": "How many 3MB 'songs' can be stored in 1TB?", "chosen": "1. Storage: 1 TB = 1,000,000 MB.\n2. Song size: 3 MB.\n3. Calculation: 1,000,000 MB / 3 MB/song = 333,333 songs.\nWe can store **~333,000 songs**.", "rejected": "1TB = 1000 GB. 1GB = 1000MB. 1000 * 1000 = 1M. 1M / 3 = 333,333. (This is correct. Rejected): 1TB = 1,000,000,000. 1,000,000,000 / 3 = 333,333,333 songs. (Unit error)."}
{"prompt": "Estimate the monthly egress cost for 1,000 users streaming 1 hour of 3 Mbps video per day.", "chosen": "1. Users: 1,000\n2. Time: 1 hour/day = 3,600 sec/day.\n3. Bitrate: 3 Mbps.\n4. Data/user/day (bits): 3,600 sec * 3 Mbps = 10,800 Mbit.\n5. Data/user/day (bytes): 10,800 / 8 bits/byte = 1,350 MBytes = 1.35 GB.\n6. Total data/day: 1,000 users * 1.35 GB/user = 1,350 GB/day.\n7. Total data/month: 1,350 GB/day * 30 days/mo = 40,500 GB = 40.5 TB.\n8. Cost (tiered): First 10TB ($0.09) + Next 30.5TB ($0.085)\n   (10,000 * 0.09) + (30,500 * 0.085) = $900 + $2,592.50 = $3,492.50.\nThe cost would be **~$3,500 per month**.", "rejected": "1000 users * 1 hour * 3 Mbps = 3000. $3000 per month."}
{"prompt": "A 100-node cluster has a 1% failure rate per node per year. How many failures per month?", "chosen": "1. Nodes: 100\n2. Failures/node/year: 0.01\n3. Total failures/year: 100 nodes * 0.01 = 1 failure/year.\n4. Failures/month: 1 / 12 = 0.083.\nThis means we'd expect **1 failure every 12 months**, or ~0.08 failures per month.", "rejected": "100 nodes * 1% = 1. We get 1 failure per node. 100 failures per year. 100 / 12 = 8.3 failures per month."}
{"prompt": "How many 50ms DB queries can a 4-core server run per second, if each query is 100% CPU?", "chosen": "1. This is a CPU-bound task.\n2. Time/query: 50ms = 0.05 seconds.\n3. Queries/core/sec: 1 sec / 0.05 sec/query = 20 QPS.\n4. Total QPS: 20 QPS/core * 4 cores = 80 QPS.\nThe server can run **80 QPS**.", "rejected": "50ms is fast. 4 cores. 4 * 1000 = 4000 QPS."}
{"prompt": "How many 50ms DB queries (I/O bound) can a 4-core server with 100 worker threads run?", "chosen": "1. This is I/O bound. Cores don't matter as much as connection/worker limits.\n2. Latency: 50ms = 0.05s.\n3. Workers: 100.\n4. Little's Law: RPS = Concurrency / Latency.\n5. Calculation: 100 workers / 0.05 sec/query = 2,000 QPS.\nAssuming the DB can handle it, the server can drive **2,000 QPS**.", "rejected": "4 cores. 50ms. 1 / 0.05 = 20. 20 * 4 = 80 QPS. (Assumes CPU bound)."}
{"prompt": "Estimate the storage for 50 million 'click' events (timestamp, user_id, item_id).", "chosen": "1. Events: 50,000,000\n2. Data per event:\n   - timestamp (TIMESTAMPTZ): 8 bytes\n   - user_id (BIGINT): 8 bytes\n   - item_id (BIGINT): 8 bytes\n3. Total per event: 8 + 8 + 8 = 24 bytes.\n4. Row/storage overhead: Let's say 100% overhead for compression, headers, etc. 24 * 2 = 48 bytes.\n5. Calculation: 50,000,000 * 48 bytes = 2,400,000,000 bytes = 2.4 GB.\nWe'd need **around 2.4 GB**.", "rejected": "A click event is 1KB. 50,000,000 * 1KB = 50GB."}
{"prompt": "A service handles 10M requests/day. What is the average RPS?", "chosen": "1. Requests/day: 10,000,000\n2. Seconds/day: 86,400\n3. Calculation: 10,000,000 / 86,400 ≈ 115.7 RPS.\nThe average is **~116 RPS**.", "rejected": "10M / 24 hours = 416,666. 416,666 / 60 min = 6,944. 6,944 / 60 sec = 11,574 RPS. (Math error)."}
{"prompt": "How many 1KB 'read' QPS can a 10Gbit/s network link support?", "chosen": "1. Link: 10 Gbit/s = 10,000 Mbit/s.\n2. Read size: 1 KB = 8 Kbit.\n3. Calculation: 10,000 Mbit/s / (8 Kbit/op) = 10,000,000 Kbit/s / 8 Kbit/op = 1,250,000 QPS.\n4. Overhead: With 10% TCP/IP overhead: 1,250,000 * 0.9 = 1,125,000.\nIt can support **~1.1-1.2 million QPS**.", "rejected": "10 Gbit/s. 1KB. 10 / 1 = 10. 10 QPS. (Unit error)."}
{"prompt": "Estimate the cost of a 20-person engineering team, avg salary $120k/year.", "chosen": "1. Salaries: 20 * $120,000 = $2,400,000 / year.\n2. Fully loaded cost: Add 40% for benefits, tax, office, software.\n3. Calculation: $2,400,000 * 1.4 = $3,360,000 / year.\n4. Monthly: $3,360,000 / 12 = $280,000 / month.\nThe team costs **~$280,000 per month**.", "rejected": "20 * 120k = $2.4M. $2.4M / 12 = $200,000/month. (Ignores loaded cost)."}
{"prompt": "How much RAM to store 1 million UUIDs in a set (in Python)?", "chosen": "1. UUIDs: 1,000,000\n2. UUID object: A Python UUID object is ~64 bytes.\n3. Set overhead: A Python set/dict has overhead. A 1M-entry dict is ~30-40MB. Let's say 40MB.\n4. Data: 1,000,000 * 64 bytes = 64,000,000 bytes = 64 MB.\n5. Total: 64 MB (data) + 40 MB (set) = 104 MB.\nWe'd need **around 100 MB** of RAM.", "rejected": "A UUID is 16 bytes. 1,000,000 * 16 = 16MB. (Ignores Python object overhead and set overhead)."}
{"prompt": "A service needs 5,000 RPS. One server does 800 RPS. How many servers?", "chosen": "1. Need: 5,000 RPS\n2. Capacity: 800 RPS/server\n3. Calculation: 5,000 / 800 = 6.25 servers.\n4. Reality: You can't have 0.25 servers. So you need 7.\n5. Redundancy (N+1): 7 + 1 = 8.\nWe need **8 servers**.", "rejected": "5,000 / 800 = 6.25. We need 6 servers. (Can't round down, no redundancy)."}
{"prompt": "How many 5KB documents can be indexed per second on 8 cores, if 1 core = 100 docs/sec?", "chosen": "1. Cores: 8\n2. Rate/core: 100 docs/sec.\n3. Calculation: 8 * 100 = 800 docs/sec.\nThe cluster can index **800 documents/sec**.", "rejected": "8 cores * 100 docs/sec = 800. 800 * 5KB = 4,000 KB/s = 4MB/s. (Answers in MB/s, not docs/sec)."}
{"prompt": "Estimate monthly egress for 10M DAU, 1KB/user/day.", "chosen": "1. DAU: 10,000,000\n2. Data/user: 1 KB\n3. Data/day: 10,000,000 * 1 KB = 10,000,000 KB = 10 GB.\n4. Data/month: 10 GB/day * 30 days/mo = 300 GB.\n5. Cost (AWS): 100GB free. 200GB * $0.09/GB = $18.\nThis would cost **~$18 per month**.", "rejected": "10M * 1KB = 10GB/day. 300GB/month. 300 * $0.09 = $27. (Ignores free tier)."}
{"prompt": "How many 2MB images can be stored on a 512GB SSD?", "chosen": "1. Storage: 512 GB = 512,000 MB.\n2. Image size: 2 MB.\n3. Calculation: 512,000 MB / 2 MB/image = 256,000 images.\n4. Overhead: OS + filesystem. Let's say 10% is used. 512 * 0.9 = 460.8 GB. 460,800 / 2 = 230,400.\nWe can store **~230,000 images**.", "rejected": "512 / 2 = 256. 256 images. (Unit error)."}
{"prompt": "A bug costs 1 hour of one engineer's time. Team of 10 cost $1.5M/year. What's the bug's cost?", "chosen": "1. Team cost/year: $1,500,000\n2. Cost/engineer: $150,000\n3. Hours/year (work): 2,000.\n4. Cost/hour/engineer: $150,000 / 2,000 = $75.\n5. Loaded cost (x1.4): $75 * 1.4 = $105.\nThe 1-hour bug costs **~$105**.", "rejected": "1 engineer, 1 hour. $150,000 / (365*24) = $17.12. (Uses 24/7/365)."}
{"prompt": "How many 50-byte 'tags' can be stored in 1MB of RAM?", "chosen": "1. RAM: 1 MB = 1,000,000 bytes.\n2. Tag size: 50 bytes.\n3. Calculation: 1,000,000 / 50 = 20,000.\n4. Overhead: Storing in a list/set adds object overhead. Say 20 bytes/obj. 1,000,000 / 70 ≈ 14,285.\nWe can store **~14,000 tags**.", "rejected": "1MB = 1024KB. 50 bytes = 0.05KB. 1024 / 0.05 = 20,480. (Overly precise, ignores overhead)."}
{"prompt": "Estimate the cost of 100,000,000 S3 'PUT' requests.", "chosen": "1. Requests: 100,000,000\n2. S3 PUT cost: $0.005 per 1,000 requests.\n3. Calculation: (100,000,000 / 1,000) * $0.005 = 100,000 * $0.005 = $500.\nThe cost is **$500**.", "rejected": "100M * $0.005 = $500,000. (Forgets 'per 1,000')."}
{"prompt": "How long to run 1 trillion iterations if one takes 1ns?", "chosen": "1. Iterations: $10^{12}$\n2. Time/iter: 1 ns = $10^{-9}$ sec.\n3. Calculation: $10^{12} * 10^{-9} = 10^3$ seconds = 1,000 seconds.\n4. Conversion: 1,000 / 60 ≈ 16.6 minutes.\nIt would take **~16.7 minutes**.", "rejected": "1 trillion * 1 = 1 trillion seconds. (Ignores 'nano')."}
{"prompt": "How many 10KB 'articles' can be stored in 1GB?", "chosen": "1. Storage: 1 GB = 1,000,000 KB.\n2. Article size: 10 KB.\n3. Calculation: 1,000,000 / 10 = 100,000.\nWe can store **100,000 articles** (before DB overhead).", "rejected": "1GB = 1,000 MB. 1,000 / 10 = 100. 100 articles. (Confuses KB and MB)."}
{"prompt": "A 10-node cluster has 1% chance of 1 node failing per day. What's the chance 1+ nodes fail in a day?", "chosen": "1. Nodes: 10\n2. P(node_fail): 0.01\n3. P(node_ok): 1 - 0.01 = 0.99.\n4. P(all_nodes_ok): $0.99^{10}$.  $0.99^2 \\approx 0.98$. $0.98^5 \\approx (1 - 0.02*5) = 0.90$. (Binomial approximation). \n   Actual: $0.99^{10} \\approx 0.904$.\n5. P(at_least_one_fails): 1 - P(all_ok) = 1 - 0.904 = 0.096.\nThere is a **~9.6% chance** of at least one failure.", "rejected": "10 nodes * 1% = 10%. (This is a decent approximation but is the 'rejected' simple answer)."}
{"prompt": "Estimate storage for 1M users, 100 'friends' each (8-byte IDs).", "chosen": "1. Users: 1,000,000\n2. Friends/user: 100\n3. Total relationships: 1M * 100 = 100,000,000.\n4. Join table: (user_id, friend_id) = 8 bytes + 8 bytes = 16 bytes.\n5. Data size: 100,000,000 * 16 bytes = 1,600,000,000 bytes = 1.6 GB.\n6. Overhead (row, page): 1.6 * 1.3 = 2.08 GB.\nWe need **~2.1 GB** for the table, plus 2x for indexes.", "rejected": "1M users * 100 friends * 8 bytes = 800MB. (Ignores the join table needs 2 IDs)."}
{"prompt": "How many 500-byte events/sec can a 1 Gbit/s link handle?", "chosen": "1. Link: 1 Gbit/s = 1,000,000,000 bit/s.\n2. Event: 500 bytes = 4,000 bits.\n3. Calculation: 1,000,000,000 / 4,000 = 250,000 events/sec.\n4. Overhead: 10% for TCP/IP. 250,000 * 0.9 = 225,000.\nIt can handle **~225,000 events/sec**.", "rejected": "1 Gbit/s = 125 MB/s. 125,000,000 / 500 = 250,000. (This is correct, but let's make rejected wrong). 1,000,000,000 / 500 = 2,000,000."}
{"prompt": "A 100,000-line codebase. 10 engineers. How many lines/engineer?", "chosen": "1. LOC: 100,000\n2. Engineers: 10\n3. Calculation: 100,000 / 10 = 10,000 lines/engineer.\nThis is an average, **10,000 LOC/engineer**, but doesn't mean 'ownership'.", "rejected": "A good engineer writes 100 lines/day. 100 * 200 days = 20,000. 10 * 20,000 = 200,000 LOC. (Answers a different question)."}
{"prompt": "Estimate monthly cost of 1,000 'always-on' t3.micro instances (1-yr RI, no upfront).", "chosen": "1. Instances: 1,000\n2. Price (t3.micro, 1-yr RI, no upfront): ~$0.0066/hour.\n3. Hours/month: 720.\n4. Cost/instance: 720 * $0.0066 ≈ $4.752.\n5. Total cost: 1,000 * $4.752 = $4,752.\nIt would cost **~$4,750 per month**.", "rejected": "t3.micro on-demand is ~$0.0104/hr * 720 = $7.488. 1000 * 7.488 = $7,488. (Uses on-demand pricing)."}
{"prompt": "How many 300KB 'product image' thumbnails can be generated from 1TB of 5MB source images?", "chosen": "1. Source data: 1 TB = 1,000,000 MB.\n2. Source image size: 5 MB.\n3. Number of images: 1,000,000 MB / 5 MB/image = 200,000 images.\n4. Thumbnails: Each image gets one thumbnail. \nSo, **200,000 thumbnails** can be generated. The 300KB is extra info.", "rejected": "1TB = 1,000,000,000 KB. 1,000,000,000 / 300 = 3,333,333 thumbnails. (Calculates how many 300KB files fit in 1TB, not how many are *generated*)."}
{"prompt": "How long to run 10 million 'npm install's if each takes 30 seconds?", "chosen": "1. Runs: 10,000,000\n2. Time/run: 30 seconds.\n3. Total time: 10,000,000 * 30 sec = 300,000,000 seconds.\n4. Conversion: 300M sec / 3600 sec/hr = 83,333 hours.\n5. Conversion: 83,333 hr / 24 hr/day = 3,472 days.\n6. Conversion: 3,472 days / 365 days/yr ≈ 9.5 years.\nThis is **9.5 years** of *serial* compute time.", "rejected": "10 million * 30 = 300 million minutes. That's centuries."}
{"prompt": "A 10-page 'user settings' form. 1 API call per field. 50ms per call. How long to save?", "chosen": "1. Pages: 10\n2. Fields/page: Let's assume 10. Total = 100 fields.\n3. API calls: 100.\n4. Latency: 50ms.\n5. Serial: If each call waits for the last, 100 * 50ms = 5,000ms = 5 seconds.\n6. Parallel: If we run 10 in parallel. 100 / 10 = 10 'batches'. 10 * 50ms = 500ms.\nThis is a bad design. A better design is one 'Save' button. 1 call * 50ms = 50ms.\nAssuming the bad design (serial), it takes **5 seconds**.", "rejected": "10 pages * 50ms = 500ms. (Ignores fields)."}
{"prompt": "Estimate storage for 100M 10-char 'coupon codes' (ASCII).", "chosen": "1. Codes: 100,000,000\n2. Size: 10 chars = 10 bytes.\n3. Overhead: Store in a DB with a primary key (say, 8 bytes). 10 + 8 = 18 bytes.\n4. Row overhead: ~24 bytes. Total = 18 + 24 = 42 bytes.\n5. Calculation: 100,000,000 * 42 bytes = 4,200,000,000 bytes = 4.2 GB.\nWe'd need **~4.2 GB** for the table, plus an index on the code.", "rejected": "100M * 10 bytes = 1,000,000,000 bytes = 1GB. (Ignores all overhead)."}
{"prompt": "How many 50KB 'user configs' can a 1TB disk hold?", "chosen": "1. Disk: 1 TB = 1,000,000,000 KB.\n2. Config: 50 KB.\n3. Calculation: 1,000,000,000 / 50 = 20,000,000.\n4. Overhead: Filesystem. 1,000,000,000 * 0.95 (5% overhead) / 50 = 19,000,000.\nWe can store **~19-20 million** configs.", "rejected": "1TB = 1,000 GB = 1,000,000 MB. 50KB = 0.05MB. 1,000,000 / 0.05 = 20,000,000. (This is correct. Rejected): 1TB / 50KB = 20,000. (Unit error)."}
{"prompt": "Estimate monthly cost of 100M 'CloudWatch' metrics (1 data point/min).", "chosen": "1. Metrics: 100,000,000\n2. This is a very high number. Standard metrics are $0.30/metric/month.\n3. Calculation: 100,000,000 * $0.30 = $30,000,000/month.\n4. This must be 'custom metrics' with 1-min resolution. The first 10,000 are $0.30. Next 240k are $0.10. Next 750k are $0.05. Over 1M are $0.025.\n5. Bulk: 99M * $0.025 = $2,475,000.\nThis is still huge. Let's re-read. '100M metrics'. Does this mean 100M *datapoints*? \nLet's assume 10,000 custom metrics. 10,000 * $0.30 = $3,000/month.\nLet's assume the prompt means 100,000 metrics. (10k * 0.3) + (90k * 0.1) = 3000 + 9000 = $12,000.\nLet's assume 100M *datapoints*. 1 datapoint/min = 43,200/month. 100M / 43,200 = 2,314 metrics. \n6. Calc 2: 2,314 metrics * $0.30/metric = $694.20.\nThis seems most plausible. **~$700/month**.", "rejected": "100 million * $0.30 = $30 million. (Misinterprets 'metrics')."}
{"prompt": "How many 1KB 'logs' per day from 100 servers, 10 logs/sec/server?", "chosen": "1. Servers: 100\n2. Rate: 10 logs/sec/server\n3. Total rate: 100 * 10 = 1,000 logs/sec.\n4. Size: 1 KB\n5. Data/sec: 1,000 logs/sec * 1 KB/log = 1,000 KB/s = 1 MB/s.\n6. Data/day: 1 MB/s * 86,400 sec/day = 86,400 MB = 86.4 GB.\nWe'd get **~86.4 GB of logs per day**.", "rejected": "100 * 10 * 1KB = 1,000 KB = 1MB. 1MB per day. (Ignores 'per sec')."}
{"prompt": "A 100MB file. 1% corrupted. How many bytes are good?", "chosen": "1. File: 100 MB\n2. Corrupted: 1%\n3. Good: 99%\n4. Calculation: 100 MB * 0.99 = 99 MB.\n**99 MB** are good.", "rejected": "100MB * 1% = 1MB. 1MB is corrupted. (Doesn't answer 'how many are good')."}
{"prompt": "Estimate p99 latency for an API with 3 serial calls: p99s are 50ms, 100ms, 30ms.", "chosen": "1. This is tricky. p99(A+B) != p99(A) + p99(B).\n2. Avg (p50): Maybe 20ms + 40ms + 10ms = 70ms.\n3. p99: The total p99 is dominated by the chance *one* of them is slow. P(any_slow) > P(one_is_slow).\n4. Upper bound: 50 + 100 + 30 = 180ms. (If they all hit p99, p99.99...).\n5. Lower bound: p99(100ms) = 100ms. (The p99 of the total must be *at least* the p99 of the slowest part).\n6. Estimate: The total p99 will be slightly higher than the max p99. Let's say **~120-130ms**.", "rejected": "50ms + 100ms + 30ms = 180ms. The p99 is 180ms."}
{"prompt": "How many 10-byte 'keys' can 1GB of RAM store in a hash map?", "chosen": "1. RAM: 1 GB = 1,000,000,000 bytes.\n2. Key: 10 bytes (as string/bytes object, maybe 40 bytes overhead in Python). Total = 50 bytes.\n3. Value: The map has to store a value. Let's say 8-byte pointer. Total = 58 bytes.\n4. Hashmap overhead: The map itself (buckets, pointers) adds overhead. A 1M entry map might be 40MB. \nLet's assume 80 bytes total per entry. \n5. Calculation: 1,000,000,000 / 80 = 12,500,000.\nWe can store **~12.5 million keys**.", "rejected": "1GB = 1,000,000,000 bytes. 1,000,000,000 / 10 = 100,000,000. 100 million keys. (Ignores *all* overhead)."}
{"prompt": "A 100GB 'users' table. 1% new users/day. How much new data/day?", "chosen": "1. Table: 100 GB.\n2. Growth: 1% / day.\n3. Calculation: 100 GB * 0.01 = 1 GB.\nWe'd add **1 GB** of new data per day (plus indexes).", "rejected": "100GB * 1% = 1MB. (Math error)."}
{"prompt": "A 500-page website. 100ms to test 1 page. How long to run the *full* test suite?", "chosen": "1. Pages: 500\n2. Time/page: 100ms = 0.1 sec.\n3. Serial time: 500 * 0.1 sec = 50 seconds.\n4. Parallel: If we run 10 tests in parallel. 500 / 10 = 50 'runs'. 50 * 0.1 sec = 5 seconds.\nAssuming 10-way parallelization, it would take **5 seconds**.", "rejected": "500 * 100ms = 50,000ms = 50 seconds. (Assumes serial)."}
{"prompt": "How many 4-core servers for 2,000 RPS, if 1 core = 50 RPS?", "chosen": "1. Need: 2,000 RPS\n2. Server capacity: 4 cores * 50 RPS/core = 200 RPS.\n3. Servers: 2,000 / 200 = 10 servers.\n4. Redundancy (N+1): 10 + 1 = 11.\nWe need **11 servers**.", "rejected": "2,000 / 50 = 40. 40 cores. 40 / 4 = 10. 10 servers. (No redundancy)."}
{"prompt": "A 50-engineer team. 10% turnover/year. How many new hires/year?", "chosen": "1. Team: 50\n2. Turnover: 10% / year.\n3. Calculation: 50 * 0.10 = 5.\nWe'd need to hire **5 new engineers per year** just to stay level.", "rejected": "10% turnover. 100% / 10% = 10. The whole team turns over every 10 years. (Doesn't answer the question)."}
{"prompt": "How many 1KB 'user profiles' can 1TB of storage hold?", "chosen": "1. Storage: 1 TB = 1,000 GB = 1,000,000 MB = 1,000,000,000 KB.\n2. Profile size: 1 KB.\n3. Calculation: 1,000,000,000 / 1 = 1,000,000,000.\nWe can store **1 billion profiles** (before DB overhead/indexes).", "rejected": "1TB = 1,000 GB. 1GB = 1,000,000 KB. 1,000 * 1,000,000 = 1,000,000,000. (This is correct. Rejected): 1TB / 1KB = 1,000. 1,000 profiles. (Unit error)."}
{"prompt": "Estimate monthly cost of 10M API Gateway requests (us-east-1).", "chosen": "1. Requests: 10,000,000\n2. Free tier: 1M (HTTP APIs). Let's assume REST API.\n3. REST API cost: $3.50 per million (first 333M).\n4. Free tier: 1M free requests. 9M billable.\n5. Calculation: 9 * $3.50 = $31.50.\nIt would cost **~$31.50 per month**.", "rejected": "10M * $3.50 = $35. (Ignores free tier)."}
{"prompt": "How many 1MB 'files' can a 256GB laptop hold?", "chosen": "1. Storage: 256 GB = 256,000 MB.\n2. File size: 1 MB.\n3. Calculation: 256,000 / 1 = 256,000.\n4. Overhead: OS, apps. Let's say 60GB is used. 196GB free. 196,000 MB / 1 = 196,000.\nWe can store **~196,000 files** on the free space.", "rejected": "256 GB / 1 MB = 256. 256 files. (Unit error)."}
{"prompt": "A 100-line function. How many unit tests?", "chosen": "1. This depends on cyclomatic complexity, not just lines.\n2. A 100-line function is huge and complex. It probably has 10-20 'paths' or 'branches'.\n3. Rule of thumb: 1 test per 'logical branch'.\n4. Estimate: A function this large probably needs **15-20 unit tests** to be well-covered.", "rejected": "One function, one test. So 1 unit test."}
{"prompt": "How many 10-char 'passwords' (lowercase) to get a 50% collision on SHA-1 (160-bit)?", "chosen": "1. This is a 'birthday problem'.\n2. Hash space: $2^{160}$.\n3. 50% collision: $n \\approx \\sqrt{2 \times 2^{160}} = \\sqrt{2^{161}} \\approx 2^{80.5}$.\n4. $2^{80}$ is an astronomically large number. $2^{10} \\approx 10^3$. $2^{80} = (2^{10})^8 \\approx (10^3)^8 = 10^{24}$.\n5. The '10-char' part is irrelevant. You need **$\\sim 2^{80}$ (or $1.2 \times 10^{24}$) hashes**.", "rejected": "10 chars. $26^{10}$. You need $26^{10}$ passwords."}
{"prompt": "How many 1KB 'read' QPS can a 1Gbit/s link *realistically* support?", "chosen": "1. Link: 1 Gbit/s = 125 MB/s (125,000 KB/s).\n2. Theoretical: 125,000 QPS.\n3. Overhead: TCP/IP + Ethernet. A 1KB payload (1024b) needs IP/TCP headers (40b) + Ethernet (14b). Total = 1078b. Overhead is 54/1078 = 5%.\n4. Packet size: A 1KB payload needs a full packet. Max packets/sec (PPS) for a 1Gbit/s NIC is ~100k-150k PPS.\n5. Bottleneck: The PPS limit is the *real* bottleneck, not bandwidth.\nIt can realistically support **~100,000 QPS**.", "rejected": "1 Gbit/s = 125,000 KB/s. So 125,000 QPS. (Ignores packet-per-second limits)."}
{"prompt": "Estimate storage for 1M users, 100KB 'profile blob' (JSON) each.", "chosen": "1. Users: 1,000,000\n2. Size: 100 KB\n3. Calculation: 1,000,000 * 100 KB = 100,000,000 KB.\n4. Conversion: 100,000,000 KB = 100,000 MB = 100 GB.\nWe'd need **100 GB** (plus DB overhead/indexes).", "rejected": "1M * 100KB = 100,000 KB = 100MB. (Math error)."}
{"prompt": "How many 100-byte 'events' per day from 10k devices, 1 event/min/device?", "chosen": "1. Devices: 10,000\n2. Rate: 1 event/min/device.\n3. Total rate: 10,000 events/min.\n4. Minutes/day: 60 * 24 = 1,440.\n5. Total events/day: 10,000 * 1,440 = 14,400,000 events/day.\n6. Size: 100 bytes.\n7. Data/day: 14,400,000 * 100 bytes = 1,440,000,000 bytes = 1.44 GB.\nWe'd get **1.44 GB of events per day**.", "rejected": "10,000 * 1 * 100 = 1,000,000 bytes = 1MB. (Ignores 'per min' and 'per day')."}
{"prompt": "How long to 'npm install' 1,000 packages (serially) if each takes 5 seconds?", "chosen": "1. Packages: 1,000\n2. Time/pkg: 5 seconds.\n3. Total time: 1,000 * 5 = 5,000 seconds.\n4. Conversion: 5,000 / 60 = 83.3 minutes.\n5. Conversion: 83.3 / 60 = 1.38 hours.\nIt would take **~1.4 hours**.", "rejected": "1,000 * 5 = 5,000 minutes. (Math error)."}
{"prompt": "How many 4-core servers for 8,000 RPS, if 1 core = 100 RPS?", "chosen": "1. Need: 8,000 RPS\n2. Server capacity: 4 cores * 100 RPS/core = 400 RPS.\n3. Servers: 8,000 / 400 = 20 servers.\n4. Redundancy (N+1): 20 + 1 = 21.\nWe need **21 servers**.", "rejected": "8,000 / 100 = 80 cores. 80 / 4 = 20. 20 servers. (No redundancy)."}
{"prompt": "Estimate storage for 1M users, 10 'bookmarks' (8-byte ID) each.", "chosen": "1. Users: 1,000,000\n2. Bookmarks/user: 10\n3. Join table: (user_id, bookmark_id) = 8 + 8 = 16 bytes.\n4. Total relationships: 1M * 10 = 10,000,000.\n5. Data size: 10,000,000 * 16 bytes = 160,000,000 bytes = 160 MB.\n6. Overhead (row, page): 160 * 1.3 = 208 MB.\nWe'd need **~200 MB** for the table, plus indexes.", "rejected": "1M users * 10 * 8 bytes = 80MB. (Ignores join table needs 2 IDs)."}
{"prompt": "How many 1KB 'read' QPS can a 10Mbit/s link support?", "chosen": "1. Link: 10 Mbit/s.\n2. Read: 1 KB = 8 Kbit.\n3. Calculation: 10 Mbit/s / 8 Kbit/op = 10,000 Kbit/s / 8 Kbit/op = 1,250 QPS.\n4. Overhead: 1,250 * 0.9 (10% overhead) = 1,125.\nIt can support **~1,125 QPS**.", "rejected": "10Mbit/s / 1KB = 10,000 QPS. (Confuses bits/bytes)."}
{"prompt": "A 1M-line codebase. 1% 'buggy' lines. How many buggy lines?", "chosen": "1. LOC: 1,000,000\n2. Bug %: 1%\n3. Calculation: 1,000,000 * 0.01 = 10,000.\nThere are **10,000 buggy lines** (as an estimate).", "rejected": "1,000,000 * 1.0 = 1,000,000. All lines are buggy."}
{"prompt": "Estimate cost of 500M S3 'GET' requests.", "chosen": "1. Requests: 500,000,000\n2. Cost: $0.0004 per 1,000.\n3. Calculation: (500,000,000 / 1,000) * $0.0004 = 500,000 * $0.0004 = $200.\nIt would cost **$200**.", "rejected": "500,000,000 * $0.0004 = $200,000. (Forgets 'per 1,000')."}
{"prompt": "How many 2MB 'React app' builds can 1TB of 'artifact' storage hold?", "chosen": "1. Storage: 1 TB = 1,000,000 MB.\n2. Build size: 2 MB.\n3. Calculation: 1,000,000 / 2 = 500,000.\nIt can hold **500,000 builds**.", "rejected": "1TB = 1,000 GB. 1,000 / 0.002 = 500,000. (This is correct. Rejected): 1TB / 2MB = 500. 500 builds. (Unit error)."}
{"prompt": "A 100ms API call. 10 retries (exponential backoff). How long until it 'fails'?", "chosen": "1. Retries: 10.\n2. Backoff: 100ms, 200ms, 400ms, 800ms, 1.6s, 3.2s, 6.4s, 12.8s, 25.6s, 51.2s.\n3. Total time: Sum(100ms...51.2s) + (10 * 100ms call time).\n4. Sum (geometric series): $0.1 * (2^{10} - 1) / (2-1) \\approx 0.1 * (1024 - 1) \\approx 102.3$ seconds.\n5. Call time: 10 * 0.1s = 1 second. \n6. Total: 102.3 + 1 = 103.3 seconds.\nIt will fail after **~103 seconds**.", "rejected": "100ms * 10 retries = 1,000ms = 1 second. (Ignores backoff)."}
{"prompt": "Estimate storage for 1M 'auth' tokens (JWTs), 250 bytes each.", "chosen": "1. Tokens: 1,000,000\n2. Size: 250 bytes.\n3. Calculation: 1,000,000 * 250 bytes = 250,000,000 bytes.\n4. Conversion: 250,000,000 bytes = 250 MB.\nWe'd need **250 MB** (e.g., in Redis).", "rejected": "1M * 250 = 250,000 MB = 250GB. (Math error)."}
{"prompt": "How many 1KB 'logs' per day from 500 servers, 5 logs/sec/server?", "chosen": "1. Servers: 500\n2. Rate: 5 logs/sec/server\n3. Total rate: 500 * 5 = 2,500 logs/sec.\n4. Size: 1 KB\n5. Data/sec: 2,500 KB/s = 2.5 MB/s.\n6. Data/day: 2.5 MB/s * 86,400 sec/day = 216,000 MB = 216 GB.\nWe'd get **216 GB of logs per day**.", "rejected": "500 * 5 * 1KB = 2,500 KB/day = 2.5MB/day. (Ignores 'per sec')."}
{"prompt": "A 10GB 'users' table. 5% churn/month. How much 'dead' data after 1 year (no deletes)?", "chosen": "1. Table: 100 GB. (Let's use 100GB, 10GB is small). 100 GB.\n2. Churn: 5% / month.\n3. After 1 mo: 100 * 0.05 = 5GB dead.\n4. After 2 mo: (100+5) * 0.05 = 5.25GB dead. Total = 10.25GB.\n5. This is compounding. Let's simplify: 5% of the *current* userbase. Let's assume 100GB is stable. 5% churn/mo = 5GB/mo.\n6. After 1 year: 5 GB/mo * 12 mo = 60 GB.\nAfter 1 year, **~60 GB** of the (now 160GB) table would be 'dead' rows.", "rejected": "10GB * 5% = 0.5GB. After 1 year, 0.5GB is dead. (Ignores 12 months)."}
{"prompt": "How many 8-core servers for 1,600 RPS, if 1 core = 50 RPS?", "chosen": "1. Need: 1,600 RPS\n2. Server capacity: 8 cores * 50 RPS/core = 400 RPS.\n3. Servers: 1,600 / 400 = 4 servers.\n4. Redundancy (N+1): 4 + 1 = 5.\nWe need **5 servers**.", "rejected": "1,600 / 50 = 32 cores. 32 / 8 = 4. 4 servers. (No redundancy)."}
{"prompt": "Estimate cost of 1B API Gateway requests (REST).", "chosen": "1. Requests: 1,000,000,000\n2. Free: 1M.\n3. Tier 1 (333M): 332M * $3.50/M = $1,162.\n4. Tier 2 (Next 667M): 667M * $2.80/M = $1,867.60.\n5. Total: $1,162 + $1,867.60 = $3,029.60.\nIt would cost **~$3,030 per month**.", "rejected": "1B = 1,000 M. 1,000 * $3.50 = $3,500. (Ignores tiered pricing)."}
{"prompt": "How many 50KB 'CSS' files can 1GB of RAM cache?", "chosen": "1. RAM: 1 GB = 1,000,000 KB.\n2. File: 50 KB.\n3. Calculation: 1,000,000 / 50 = 20,000.\n4. Overhead: Cache (e.g. Varnish, Nginx) has overhead. Let's say 10%.\n5. Realistic: 20,000 * 0.9 = 18,000.\nWe can cache **~18,000 files**.", "rejected": "1GB = 1,024 MB. 1,024,000 KB. 1,024,000 / 50 = 20,480. (Overly precise, ignores overhead)."}
{"prompt": "A 1M-line codebase. 100-line 'PRs'. How many PRs?", "chosen": "1. LOC: 1,000,000\n2. PR size: 100 lines.\n3. This assumes the code was all built with 100-line PRs, which is false. \n4. Let's assume 'net' 100 lines (adds 120, deletes 20). \n5. Calculation: 1,000,000 / 100 = 10,000.\nThis implies **10,000 PRs** created the *current* code, which is a rough estimate.", "rejected": "You can't know. A PR could be 1 line or 10,000 lines. It's impossible to estimate."}
{"prompt": "How many 1KB 'key-value' pairs in 100GB of 'B-Tree' (DB table)?", "chosen": "1. Storage: 100 GB.\n2. Data size: 1 KB.\n3. Overhead: B-Trees have ~30% overhead (internal nodes, page fill). Total size = 1.3 KB.\n4. Calculation: 100 GB = 100,000,000 KB.\n5. 100,000,000 KB / 1.3 KB/pair = 76,923,000.\nWe can store **~77 million pairs**.", "rejected": "100GB = 100,000,000 KB. 100,000,000 / 1 = 100,000,000. 100 million pairs. (Ignores all overhead)."}
{"prompt": "Estimate cost of 100TB 'EFS' (Standard) storage.", "chosen": "1. Storage: 100 TB = 100,000 GB.\n2. Cost: $0.30 per GB-month.\n3. Calculation: 100,000 GB * $0.30/GB = $30,000.\nIt would cost **$30,000 per month**.", "rejected": "EFS is cheap, like S3. $0.023/GB. 100,000 * 0.023 = $2,300. (Uses S3 pricing)."}
{"prompt": "How long to 'git clone' a 1GB repo over 50 Mbps?", "chosen": "1. Data: 1 GB.\n2. Speed: 50 Mbps = 6.25 MB/s.\n3. Units: 1 GB = 1,000 MB.\n4. Calculation: 1,000 MB / 6.25 MB/s = 160 seconds.\n5. Conversion: 160 / 60 = 2.66 minutes.\nIt would take **~2.7 minutes**.", "rejected": "1GB = 1000MB. 50Mbps. 1000 / 50 = 20 seconds. (Confuses MB and Mb)."}
{"prompt": "A 100-node 'Spark' cluster. 10-minute job. 1 node fails. How much time lost?", "chosen": "1. Job: 10 minutes.\n2. Failure: 1 node fails. Spark is resilient. \n3. Lost work: The 'tasks' on that node fail. They are ~1/100 of the work.\n4. Rerun: The driver reschedules those tasks (say, 1/100 of 10 mins = 6 seconds of work) on other nodes.\n5. Overhead: There's overhead in detecting failure and rescheduling (~30-60s).\n6. Total: The 10-min job will now take **~11 minutes** (10m + 60s overhead).", "rejected": "1 node failed. The whole job fails. You lost 10 minutes and have to restart. 20 minutes total."}
{"prompt": "How many 500-byte 'events' per day from 10k devices, 1 event/sec/device?", "chosen": "1. Devices: 10,000\n2. Rate: 1 event/sec/device\n3. Total rate: 10,000 events/sec.\n4. Size: 500 bytes.\n5. Data/sec: 10,000 * 500 bytes = 5,000,000 bytes/sec = 5 MB/s.\n6. Data/day: 5 MB/s * 86,400 sec/day = 432,000 MB = 432 GB.\nWe'd get **432 GB of events per day**.", "rejected": "10,000 * 1 * 500 = 5MB/day. (Ignores 'per sec')."}
{"prompt": "Estimate storage for 1B 'user_actions' (user_id, action_type_id).", "chosen": "1. Rows: 1,000,000,000\n2. Data: user_id (8 bytes) + action_type_id (4 bytes, INT) = 12 bytes.\n3. Row overhead: ~24 bytes.\n4. Total/row: 12 + 24 = 36 bytes.\n5. Data size: 1,000,000,000 * 36 = 36,000,000,000 bytes = 36 GB.\n6. Fill factor (90%): 36 / 0.9 = 40 GB.\nWe'd need **~40 GB** for the table, plus indexes.", "rejected": "1B * (8+4) = 12GB. (Ignores all overhead)."}
{"prompt": "How many 1ms 'tasks' can 8 cores process/sec (100% CPU)?", "chosen": "1. Cores: 8\n2. Time/task: 1ms = 0.001 sec.\n3. Rate/core: 1 / 0.001 = 1,000 tasks/sec.\n4. Total rate: 8 * 1,000 = 8,000 tasks/sec.\nThe server can process **8,000 tasks/sec**.", "rejected": "8 cores * 1ms = 8ms. (Nonsensical calculation)."}
{"prompt": "A 100-user 'SaaS' app. $20/user/month. What is the ARR (Annual Recurring Revenue)?", "chosen": "1. Users: 100\n2. MRR: 100 * $20 = $2,000 / month.\n3. ARR: $2,000 / month * 12 months = $24,000.\nThe ARR is **$24,000**.", "rejected": "100 * 20 = $2,000. The ARR is $2,000. (Gives MRR, not ARR)."}
{"prompt": "How many 2KB 'objects' can 1TB of S3 hold?", "chosen": "1. Storage: 1 TB = 1,000,000,000 KB.\n2. Object: 2 KB.\n3. Calculation: 1,000,000,000 / 2 = 500,000,000.\nS3 has minimal overhead. We can store **500 million objects**.", "rejected": "1TB = 1,000 GB. 1GB = 1,000,000 KB. 1,000,000 / 2 = 500,000. 1,000 * 500,000 = 500,000,000. (This is correct. Rejected): 1TB / 2KB = 500,000. (Unit error)."}
{"prompt": "How long to 'grep' 1TB of logs on SSD (3 GB/s) if 'grep' runs at 500 MB/s?", "chosen": "1. Data: 1 TB = 1,000 GB.\n2. Bottleneck: The 'grep' (CPU) is slower (500 MB/s) than the disk (3 GB/s).\n3. Speed: 500 MB/s = 0.5 GB/s.\n4. Calculation: 1,000 GB / 0.5 GB/s = 2,000 seconds.\n5. Conversion: 2,000 / 60 = 33.3 minutes.\nIt would take **~33 minutes**.", "rejected": "SSD is 3 GB/s. 1,000 GB / 3 GB/s = 333 seconds. (Ignores CPU bottleneck)."}
{"prompt": "A 100ms 'save' op. How many can be run *serially* in 1 minute?", "chosen": "1. Time: 1 minute = 60 seconds = 60,000 ms.\n2. Op time: 100 ms.\n3. Calculation: 60,000 / 100 = 600.\nYou can run **600 ops** serially.", "rejected": "100ms = 0.1s. 1 / 0.1 = 10. 10 ops/sec. 10 * 60 = 600. (This is correct. Rejected): 60,000 * 100 = 6,000,000."}
{"prompt": "How many 2-core 'CI runner' VMs for 100 'jobs'/hour, 10 min/job?", "chosen": "1. Jobs/hr: 100\n2. Time/job: 10 min.\n3. Total compute needed: 100 * 10 = 1,000 job-minutes / hour.\n4. Compute available: A VM can run 60 minutes / hour.\n5. VMs needed: 1,000 / 60 = 16.66... \nWe need **17 VMs**.", "rejected": "100 jobs / 10 min = 10. We need 10 VMs."}
{"prompt": "Estimate storage for 1M 'git commits' (500 bytes metadata each).", "chosen": "1. Commits: 1,000,000\n2. Size: 500 bytes.\n3. Calculation: 1,000,000 * 500 = 500,000,000 bytes = 500 MB.\nThis is *just* the commit objects. The diffs/blobs are separate. The metadata is **500 MB**.", "rejected": "A commit is 10KB. 1M * 10KB = 10GB."}
{"prompt": "A 10GB 'log file'. 100 lines/MB. How many lines?", "chosen": "1. File: 10 GB = 10,000 MB.\n2. Density: 100 lines/MB.\n3. Calculation: 10,000 MB * 100 lines/MB = 1,000,000 lines.\nThe file has **1 million lines**.", "rejected": "10GB * 100 = 1,000. 1,000 lines. (Unit error)."}
{"prompt": "How many 10-char 'keys' (ASCII) can 1MB hold?", "chosen": "1. RAM: 1 MB = 1,000,000 bytes.\n2. Key: 10 bytes.\n3. Overhead: Stored in a list/set. Say 30 bytes overhead per string obj. Total = 40 bytes.\n4. Calculation: 1,000,000 / 40 = 25,000.\nWe can hold **~25,000 keys**.", "rejected": "1,000,000 / 10 = 100,000. (Ignores all overhead)."}
{"prompt": "Estimate storage for 100k 'products' (10KB JSON blob each).", "chosen": "1. Products: 100,000\n2. Size: 10 KB\n3. Calculation: 100,000 * 10 KB = 1,000,000 KB = 1 GB.\nWe'd need **1 GB** (plus indexes, e.g. in MongoDB).", "rejected": "100,000 * 10 = 1,000,000. 1MB. (Unit error)."}
{"prompt": "How long to 'zlib' compress 1TB (HDD, 100 MB/s) if 'zlib' runs at 50 MB/s?", "chosen": "1. Data: 1 TB = 1,000,000 MB.\n2. Bottleneck: 'zlib' (CPU) at 50 MB/s is slower than disk (100 MB/s).\n3. Speed: 50 MB/s.\n4. Time: 1,000,000 MB / 50 MB/s = 20,000 seconds.\n5. Conversion: 20,000 / 3600 = 5.55 hours.\nIt would take **~5.5 hours**.", "rejected": "Disk is 100 MB/s. 1,000,000 / 100 = 10,000 seconds = 2.8 hours. (Ignores CPU bottleneck)."}
{"prompt": "A 100-node 'k8s' cluster. 50 pods/node. How many pods?", "chosen": "1. Nodes: 100\n2. Pods/node: 50\n3. Calculation: 100 * 50 = 5,000.\nThere are **5,000 pods** in the cluster.", "rejected": "100 + 50 = 150. 150 pods."}
{"prompt": "How many 1KB 'items' can 1TB hold?", "chosen": "1. Storage: 1 TB = 1,000,000,000 KB.\n2. Item: 1 KB.\n3. Calculation: 1,000,000,000 / 1 = 1,000,000,000.\nIt can hold **1 billion items**.", "rejected": "1TB = 1,000 GB. 1GB = 1,000,000 KB. 1,000 * 1,000,000 = 1,000,000,000. (This is correct. Rejected): 1TB / 1KB = 1,000,000. (Unit error)."}
{"prompt": "Estimate cost of 2B S3 'PUT' requests.", "chosen": "1. Requests: 2,000,000,000\n2. Cost: $0.005 per 1,000.\n3. Calculation: (2,000,000,000 / 1,000) * $0.005 = 2,000,000 * $0.005 = $10,000.\nThe cost is **$10,000**.", "rejected": "2B * 0.005 = $10,000,000. (Forgets 'per 1,000')."}
{"prompt": "How many 10ms 'jobs' can 1 core run/sec (100% CPU)?", "chosen": "1. Core: 1\n2. Time/job: 10ms = 0.01 sec.\n3. Rate: 1 / 0.01 = 100 jobs/sec.\nThe core can run **100 jobs/sec**.", "rejected": "10ms * 1 = 10. 10 jobs."}
{"prompt": "A 10k-user app. $5/user/mo. What's the ARR?", "chosen": "1. Users: 10,000\n2. MRR: 10,000 * $5 = $50,000 / month.\n3. ARR: $50,000 * 12 = $600,000.\nThe ARR is **$600,000**.", "rejected": "10,000 * 5 = $50,000. (Gives MRR)."}
{"prompt": "How many 1GB 'files' can 1PB hold?", "chosen": "1. Storage: 1 PB = 1,000 TB = 1,000,000 GB.\n2. File: 1 GB.\n3. Calculation: 1,000,000 / 1 = 1,000,000.\nIt can hold **1 million files**.", "rejected": "1PB / 1GB = 1,000. (Unit error)."}
{"prompt": "Estimate p95 of 2 serial calls: p95(A)=100ms, p95(B)=100ms.", "chosen": "1. Avg (p50): Maybe 40ms + 40ms = 80ms.\n2. p95: The total will be > 100ms. If A=p95 and B=p50: 100+40 = 140ms. \n3. P(both_ok) = 0.95 * 0.95 = 0.9025. This is P(total < p95), so p90.\n4. The p95 will be higher. Let's estimate **~140-150ms**.", "rejected": "100ms + 100ms = 200ms."}
{"prompt": "How many 50KB 'images' can 100TB hold?", "chosen": "1. Storage: 100 TB = 100,000,000,000 KB.\n2. Image: 50 KB.\n3. Calculation: 100,000,000,000 / 50 = 2,000,000,000.\nIt can hold **2 billion images**.", "rejected": "100TB / 50KB = 2,000. (Unit error)."}
{"prompt": "A 100MB 'payload'. How long to send over 100 Mbps?", "chosen": "1. Data: 100 MB.\n2. Speed: 100 Mbps = 12.5 MB/s.\n3. Calculation: 100 MB / 12.5 MB/s = 8 seconds.\nIt will take **8 seconds**.", "rejected": "100MB / 100Mbps = 1 second. (Confuses MB and Mb)."}
{"prompt": "How many 1KB 'logs' per day from 1k servers, 1 log/sec/server?", "chosen": "1. Servers: 1,000\n2. Rate: 1 log/sec/server\n3. Total rate: 1,000 logs/sec.\n4. Size: 1 KB\n5. Data/sec: 1,000 KB/s = 1 MB/s.\n6. Data/day: 1 MB/s * 86,400 sec/day = 86,400 MB = 86.4 GB.\nWe'd get **~86.4 GB/day**.", "rejected": "1000 * 1 * 1KB = 1MB/day. (Ignores 'per sec')."}
{"prompt": "How many 10-char 'keys' (ASCII) can 10MB hold?", "chosen": "1. RAM: 10 MB = 10,000,000 bytes.\n2. Key: 10 bytes.\n3. Overhead: ~30 bytes/obj. Total = 40 bytes.\n4. Calculation: 10,000,000 / 40 = 250,000.\nWe can hold **~250,000 keys**.", "rejected": "10,000,000 / 10 = 1,000,000. (Ignores overhead)."}
{"prompt": "Estimate storage for 10M 'products' (5KB JSON blob each).", "chosen": "1. Products: 10,000,000\n2. Size: 5 KB\n3. Calculation: 10,000,000 * 5 KB = 50,000,000 KB = 50 GB.\nWe'd need **50 GB** (plus indexes).", "rejected": "10M * 5KB = 50,000 KB = 50MB. (Math error)."}
{"prompt": "A 100-node 'k8s' cluster. 10% 'fail' per year. How many failures/month?", "chosen": "1. Nodes: 100\n2. Fail rate: 10% / year.\n3. Total failures/year: 100 * 0.10 = 10 failures/year.\n4. Failures/month: 10 / 12 = 0.83.\nWe'd expect **~1 failure per month**.", "rejected": "100 * 10% = 10. 10 failures per month."}
{"prompt": "How many 1GB 'files' can 1TB hold?", "chosen": "1. Storage: 1 TB = 1,000 GB.\n2. File: 1 GB.\n3. Calculation: 1,000 / 1 = 1,000.\nIt can hold **1,000 files**.", "rejected": "1TB / 1GB = 1. 1 file. (Unit error)."}
{"prompt": "A 1M-user app. $10/user/mo. What's the ARR?", "chosen": "1. Users: 1,000,000\n2. MRR: 1,000,000 * $10 = $10,000,000 / month.\n3. ARR: $10,000,000 * 12 = $120,000,000.\nThe ARR is **$120 Million**.", "rejected": "1M * 10 = $10M. (Gives MRR)."}
{"prompt": "How many 1KB 'files' can 1MB hold?", "chosen": "1. Storage: 1 MB = 1,000 KB.\n2. File: 1 KB.\n3. Calculation: 1,000 / 1 = 1,000.\nIt can hold **1,000 files** (minus filesystem overhead).", "rejected": "1MB / 1KB = 1. 1 file. (Unit error)."}
{"prompt": "Estimate cost of 1B S3 'GET' requests.", "chosen": "1. Requests: 1,000,000,000\n2. Cost: $0.0004 per 1,000.\n3. Calculation: (1,000,000,000 / 1,000) * $0.0004 = 1,000,000 * $0.0004 = $400.\nIt would cost **$400**.", "rejected": "1B * 0.0004 = $400,000. (Forgets 'per 1,000')."}
{"prompt": "A 5MB 'image'. How long to send over 1 Gbit/s?", "chosen": "1. Data: 5 MB.\n2. Speed: 1 Gbit/s = 125 MB/s.\n3. Calculation: 5 MB / 125 MB/s = 0.04 seconds.\n4. Conversion: 0.04s = 40 ms.\nIt will take **40 milliseconds**.", "rejected": "5MB / 1 Gbit/s = 5 seconds. (Unit error)."}
{"prompt": "How many 1KB 'logs' per day from 10k servers, 1 log/min/server?", "chosen": "1. Servers: 10,000\n2. Rate: 1 log/min/server\n3. Total rate: 10,000 logs/min.\n4. Data/min: 10,000 * 1 KB = 10,000 KB/min = 10 MB/min.\n5. Data/day: 10 MB/min * (60*24) min/day = 10 * 1,440 = 14,400 MB = 14.4 GB.\nWe'd get **14.4 GB/day**.", "rejected": "10,000 * 1 * 1KB = 10MB/day. (Ignores 'per min')."}
{"prompt": "A 100GB 'DB'. 10% 'hot'. How much RAM to cache?", "chosen": "1. DB: 100 GB.\n2. Hot set: 10%.\n3. Calculation: 100 GB * 0.10 = 10 GB.\n4. Overhead (Redis): 25%. 10 * 1.25 = 12.5 GB.\nWe'd need **~12.5 GB** of RAM.", "rejected": "100GB * 10% = 10%. 10% of RAM. (Nonsensical)."}
{"prompt": "Estimate storage for 1M 'users' (1KB each) with 3x replication.", "chosen": "1. Users: 1,000,000\n2. Size: 1 KB\n3. Data: 1,000,000 * 1 KB = 1 GB.\n4. Replication: 3x.\n5. Total: 1 GB * 3 = 3 GB.\nWe'd need **3 GB** of total storage.", "rejected": "1M * 1KB = 1GB. (Ignores replication)."}
{"prompt": "How many 4-core servers for 4,000 RPS, if 1 core = 100 RPS?", "chosen": "1. Need: 4,000 RPS\n2. Server: 4 * 100 = 400 RPS.\n3. Servers: 4,000 / 400 = 10.\n4. Redundancy: 10 + 1 = 11.\nWe need **11 servers**.", "rejected": "4,000 / 100 = 40 cores. 40 / 4 = 10. 10 servers. (No redundancy)."}
{"prompt": "A 1M-user app. 1% churn/mo. How many users lost/year?", "chosen": "1. Users: 1,000,000\n2. Churn/mo: 1% = 1,000,000 * 0.01 = 10,000 users/mo.\n3. Churn/yr: 10,000 * 12 = 120,000.\nThis is a simple model. A compounding model: $1M * (1 - 0.01)^{12} = 1M * 0.886 = 113,600$ lost.\nLet's use the simple one: **120,000 users/year**.", "rejected": "1% churn * 12 = 12%. 1M * 0.12 = 120,000. (This is correct. Rejected): 1% churn. 1,000,000 * 0.01 = 10,000 users. (Ignores 'per year')."}
{"prompt": "How many 1KB 'files' can 1TB hold?", "chosen": "1. Storage: 1 TB = 1,000,000,000 KB.\n2. File: 1 KB.\n3. Calculation: 1,000,000,000 / 1 = 1,000,000,000.\nIt can hold **1 billion files**.", "rejected": "1TB / 1KB = 1,000,000. (Unit error)."}
{"prompt": "Estimate cost of 1T 'PUT' requests to S3.", "chosen": "1. Requests: 1 Trillion = 1,000,000 Million.\n2. Cost: $0.005 per 1,000.\n3. Calculation: (1,000,000,000,000 / 1,000) * $0.005 = 1,000,000,000 * $0.005 = $5,000,000.\nThe cost is **$5 Million**.", "rejected": "1T * 0.005 = 5B. $5 Billion. (Forgets 'per 1,000')."}
{"prompt": "How many 5ms 'tasks' can 16 cores run/sec (100% CPU)?", "chosen": "1. Cores: 16\n2. Time/task: 5ms = 0.005 sec.\n3. Rate/core: 1 / 0.005 = 200 tasks/sec.\n4. Total rate: 16 * 200 = 3,200 tasks/sec.\nThe server can run **3,200 tasks/sec**.", "rejected": "16 * 5 = 80. 80 tasks."}
{"prompt": "A 100k-user app. $1/user/mo. What's the ARR?", "chosen": "1. Users: 100,000\n2. MRR: 100,000 * $1 = $100,000 / month.\n3. ARR: $100,000 * 12 = $1,200,000.\nThe ARR is **$1.2 Million**.", "rejected": "100,000 * 1 = $100,000. (Gives MRR)."}
{"prompt": "How many 1MB 'files' can 1PB hold?", "chosen": "1. Storage: 1 PB = 1,000,000,000 MB.\n2. File: 1 MB.\n3. Calculation: 1,000,000,000 / 1 = 1,000,000,000.\nIt can hold **1 billion files**.", "rejected": "1PB / 1MB = 1,000,000. (Unit error)."}
{"prompt": "A 10MB 'file'. How long to send over 10 Gbit/s?", "chosen": "1. Data: 10 MB.\n2. Speed: 10 Gbit/s = 1,250 MB/s.\n3. Calculation: 10 MB / 1,250 MB/s = 0.008 seconds.\n4. Conversion: 0.008s = 8 ms.\nIt will take **8 milliseconds**.", "rejected": "10MB / 10Gbit/s = 1 second. (Unit error)."}
{"prompt": "How many 1KB 'logs' per day from 10k servers, 10 log/min/server?", "chosen": "1. Servers: 10,000\n2. Rate: 10 logs/min/server\n3. Total rate: 100,000 logs/min.\n4. Data/min: 100,000 * 1 KB = 100,000 KB/min = 100 MB/min.\n5. Data/day: 100 MB/min * 1,440 min/day = 144,000 MB = 144 GB.\nWe'd get **144 GB/day**.", "rejected": "10,000 * 10 * 1KB = 100MB/day. (Ignores 'per min')."}
{"prompt": "A 500GB 'DB'. 2% 'hot'. How much RAM to cache?", "chosen": "1. DB: 500 GB.\n2. Hot set: 2%.\n3. Calculation: 500 GB * 0.02 = 10 GB.\n4. Overhead (Redis): 25%. 10 * 1.25 = 12.5 GB.\nWe'd need **~12.5 GB** of RAM.", "rejected": "500 * 2 = 1000. 1000GB of RAM."}
{"prompt": "Estimate storage for 10M 'users' (1KB each) with 2x replication.", "chosen": "1. Users: 10,000,000\n2. Size: 1 KB\n3. Data: 10,000,000 * 1 KB = 10 GB.\n4. Replication: 2x.\n5. Total: 10 GB * 2 = 20 GB.\nWe'd need **20 GB** of total storage.", "rejected": "10M * 1KB = 10GB. (Ignores replication)."}
{"prompt": "How many 8-core servers for 3,200 RPS, if 1 core = 50 RPS?", "chosen": "1. Need: 3,200 RPS\n2. Server: 8 * 50 = 400 RPS.\n3. Servers: 3,200 / 400 = 8.\n4. Redundancy: 8 + 1 = 9.\nWe need **9 servers**.", "rejected": "3,200 / 50 = 64 cores. 64 / 8 = 8. 8 servers. (No redundancy)."}
{"prompt": "A 5M-user app. 2% churn/mo. How many users lost/year?", "chosen": "1. Users: 5,000,000\n2. Churn/mo: 2% = 5,000,000 * 0.02 = 100,000 users/mo.\n3. Churn/yr: 100,000 * 12 = 1,200,000.\nWe'd lose **~1.2 million users/year**.", "rejected": "5M * 2% = 100,000. (Ignores 'per year')."}
{"prompt": "How many 1B 'files' can 1 Exabyte (EB) hold?", "chosen": "1. Storage: 1 EB = 1,000 PB = 1,000,000 TB = 1,000,000,000 GB.\n2. File: 1 GB.\n3. Calculation: 1,000,000,000 / 1 = 1,000,000,000.\nIt can hold **1 billion 1GB files**.", "rejected": "1 EB / 1 GB = 1,000,000. (Unit error)."}
{"prompt": "Estimate cost of 500B S3 'GET' requests.", "chosen": "1. Requests: 500,000,000,000 (500B).\n2. Cost: $0.0004 per 1,000.\n3. Calculation: (500,000,000,000 / 1,000) * $0.0004 = 500,000,000 * $0.0004 = $200,000.\nIt would cost **$200,000**.", "rejected": "500B * 0.0004 = $200M. (Forgets 'per 1,000')."}
{"prompt": "A 1KB 'file'. How long to send over 1 Gbit/s?", "chosen": "1. Data: 1 KB = 8 Kbit.\n2. Speed: 1 Gbit/s = 1,000,000 Kbit/s.\n3. Calculation: 8 / 1,000,000 = 0.000008 seconds.\n4. Conversion: 8 microseconds (µs).\n5. Reality: Network latency (1ms) dominates. It will take **~1 ms**.", "rejected": "1 Gbit/s / 1KB = 1,000,000. 1 million seconds."}
{"prompt": "How many 1KB 'logs' per day from 10k servers, 10 log/sec/server?", "chosen": "1. Servers: 10,000\n2. Rate: 10 logs/sec/server\n3. Total rate: 100,000 logs/sec.\n4. Size: 1 KB\n5. Data/sec: 100,000 KB/s = 100 MB/s.\n6. Data/day: 100 MB/s * 86,400 sec/day = 8,640,000 MB = 8.64 TB.\nWe'd get **8.64 TB/day**.", "rejected": "10,000 * 10 * 1KB = 100MB/day. (Ignores 'per sec')."}
{"prompt": "A 2TB 'DB'. 1% 'hot'. How much RAM to cache?", "chosen": "1. DB: 2 TB.\n2. Hot set: 1%.\n3. Calculation: 2 TB * 0.01 = 0.02 TB = 20 GB.\n4. Overhead: 25%. 20 * 1.25 = 25 GB.\nWe'd need **~25 GB** of RAM.", "rejected": "2TB * 1% = 0.02 GB. 20MB."}
{"prompt": "Estimate storage for 100M 'users' (1KB each) with 3x replication.", "chosen": "1. Users: 100,000,000\n2. Size: 1 KB\n3. Data: 100,000,000 * 1 KB = 100 GB.\n4. Replication: 3x.\n5. Total: 100 GB * 3 = 300 GB.\nWe'd need **300 GB** of total storage.", "rejected": "100M * 1KB = 100GB. (Ignores replication)."}
{"prompt": "How many 16-core servers for 8,000 RPS, if 1 core = 100 RPS?", "chosen": "1. Need: 8,000 RPS\n2. Server: 16 * 100 = 1,600 RPS.\n3. Servers: 8,000 / 1,600 = 5.\n4. Redundancy: 5 + 1 = 6.\nWe need **6 servers**.", "rejected": "8,000 / 100 = 80 cores. 80 / 16 = 5. 5 servers. (No redundancy)."}
{"prompt": "A 10M-user app. 1% churn/mo. How many users lost/year?", "chosen": "1. Users: 10,000,000\n2. Churn/mo: 1% = 100,000 users/mo.\n3. Churn/yr: 100,000 * 12 = 1,200,000.\nWe'd lose **~1.2 million users/year**.", "rejected": "10M * 1% = 100,000. (Ignores 'per year')."}
{"prompt": "How many 10MB 'files' can 1TB hold?", "chosen": "1. Storage: 1 TB = 1,000,000 MB.\n2. File: 10 MB.\n3. Calculation: 1,000,000 / 10 = 100,000.\nIt can hold **100,000 files**.", "rejected": "1TB / 10MB = 100. (Unit error)."}
{"prompt": "Estimate cost of 10T 'GET' requests from S3.", "chosen": "1. Requests: 10 Trillion = 10,000,000 Million.\n2. Cost: $0.0004 per 1,000.\n3. Calculation: (10,000,000,000,000 / 1,000) * $0.0004 = 10,000,000,000 * $0.0004 = $4,000,000.\nIt would cost **$4 Million**.", "rejected": "10T * 0.0004 = 4B. $4 Billion. (Forgets 'per 1,000')."}
{"prompt": "A 100KB 'file'. How long to send over 1 Gbit/s?", "chosen": "1. Data: 100 KB = 0.8 Mbit.\n2. Speed: 1 Gbit/s = 1,000 Mbit/s.\n3. Calculation: 0.8 / 1,000 = 0.0008 seconds = 0.8 ms.\n4. Reality: Latency dominates. It will take **~1-2 ms**.", "rejected": "1 Gbit/s / 100KB = 10,000. 10,000 seconds."}
{"prompt": "How many 500-byte 'logs' per day from 1k servers, 10 log/sec/server?", "chosen": "1. Servers: 1,000\n2. Rate: 10 logs/sec/server\n3. Total rate: 10,000 logs/sec.\n4. Size: 500 bytes.\n5. Data/sec: 10,000 * 500 B = 5,000,000 B/s = 5 MB/s.\n6. Data/day: 5 MB/s * 86,400 = 432,000 MB = 432 GB.\nWe'd get **432 GB/day**.", "rejected": "1,000 * 10 * 500 = 5MB/day. (Ignores 'per sec')."}
{"prompt": "A 1TB 'DB'. 5% 'hot'. How much RAM to cache?", "chosen": "1. DB: 1 TB.\n2. Hot set: 5%.\n3. Calculation: 1 TB * 0.05 = 0.05 TB = 50 GB.\n4. Overhead: 25%. 50 * 1.25 = 62.5 GB.\nWe'd need **~62.5 GB** of RAM.", "rejected": "1TB * 5% = 5GB."}
{"prompt": "Estimate storage for 10M 'users' (1KB each) with 5x replication.", "chosen": "1. Users: 10,000,000\n2. Size: 1 KB\n3. Data: 10,000,000 * 1 KB = 10 GB.\n4. Replication: 5x.\n5. Total: 10 GB * 5 = 50 GB.\nWe'd need **50 GB** of total storage.", "rejected": "10M * 1KB = 10GB. (Ignores replication)."}
{"prompt": "How many 8-core servers for 8,000 RPS, if 1 core = 200 RPS?", "chosen": "1. Need: 8,000 RPS\n2. Server: 8 * 200 = 1,600 RPS.\n3. Servers: 8,000 / 1,600 = 5.\n4. Redundancy: 5 + 1 = 6.\nWe need **6 servers**.", "rejected": "8,000 / 200 = 40 cores. 40 / 8 = 5. 5 servers. (No redundancy)."}
{"prompt": "A 1M-user app. 0.5% churn/mo. How many users lost/year?", "chosen": "1. Users: 1,000,000\n2. Churn/mo: 0.5% = 5,000 users/mo.\n3. Churn/yr: 5,000 * 12 = 60,000.\nWe'd lose **~60,000 users/year**.", "rejected": "1M * 0.5% = 5,000. (Ignores 'per year')."}
{"prompt": "How many 1KB 'files' can 1 Exabyte (EB) hold?", "chosen": "1. Storage: 1 EB = 10^{18} bytes = 10^{15} KB.\n2. File: 1 KB.\n3. Calculation: $10^{15} / 1 = 10^{15}$.\nIt can hold **1 quadrillion files**.", "rejected": "1 EB / 1KB = 1,000,000,000. (Unit error)."}
{"prompt": "A 100KB 'file'. How long to send over 10 Gbit/s?", "chosen": "1. Data: 100 KB = 0.8 Mbit.\n2. Speed: 10 Gbit/s = 10,000 Mbit/s.\n3. Calculation: 0.8 / 10,000 = 0.00008 seconds = 80 µs.\n4. Reality: Latency dominates. It will take **~1-2 ms**.", "rejected": "10 Gbit/s / 100KB = 100,000. 100,000 seconds."}
{"prompt": "How many 1KB 'logs' per day from 10k servers, 1 log/sec/server?", "chosen": "1. Servers: 10,000\n2. Rate: 1 log/sec/server\n3. Total rate: 10,000 logs/sec.\n4. Size: 1 KB\n5. Data/sec: 10,000 KB/s = 10 MB/s.\n6. Data/day: 10 MB/s * 86,400 = 864,000 MB = 864 GB.\nWe'd get **864 GB/day**.", "rejected": "10,000 * 1 * 1KB = 10MB/day. (Ignores 'per sec')."}
{"prompt": "A 100GB 'DB'. 20% 'hot'. How much RAM to cache?", "chosen": "1. DB: 100 GB.\n2. Hot set: 20%.\n3. Calculation: 100 GB * 0.20 = 20 GB.\n4. Overhead: 25%. 20 * 1.25 = 25 GB.\nWe'd need **~25 GB** of RAM.", "rejected": "100GB * 20 = 2000GB."}
{"prompt": "Estimate storage for 1M 'users' (1KB each) with 10x replication.", "chosen": "1. Users: 1,000,000\n2. Size: 1 KB\n3. Data: 1,000,000 * 1 KB = 1 GB.\n4. Replication: 10x.\n5. Total: 1 GB * 10 = 10 GB.\nWe'd need **10 GB** of total storage.", "rejected": "1M * 1KB = 1GB. (Ignores replication)."}
{"prompt": "How many 4-core servers for 1,600 RPS, if 1 core = 200 RPS?", "chosen": "1. Need: 1,600 RPS\n2. Server: 4 * 200 = 800 RPS.\n3. Servers: 1,600 / 800 = 2.\n4. Redundancy: 2 + 1 = 3.\nWe need **3 servers**.", "rejected": "1,600 / 200 = 8 cores. 8 / 4 = 2. 2 servers. (No redundancy)."}
{"prompt": "A 10M-user app. 10% churn/mo. How many users left after 1 year?", "chosen": "1. Users: 10,000,000\n2. Churn/mo: 10% (0.10).\n3. P(remain) = $0.90^{12}$. $0.9^{12} \\approx 0.282$.\n4. Users left: 10,000,000 * 0.282 = 2,820,000.\n**~2.8 million users** would be left (assuming no new growth).", "rejected": "10% * 12 = 120%. 1M - 120% = -200,000. You have negative users. (Wrongly adds percentages)."}
{"prompt": "How many 1MB 'files' can 1EB hold?", "chosen": "1. Storage: 1 EB = 10^{18} bytes = 10^{12} MB.\n2. File: 1 MB.\n3. Calculation: $10^{12} / 1 = 10^{12}$.\nIt can hold **1 trillion files**.", "rejected": "1 EB / 1MB = 1,000,000,000. (Unit error)."}
{"prompt": "A 1MB 'file'. How long to send over 10 Gbit/s?", "chosen": "1. Data: 1 MB = 8 Mbit.\n2. Speed: 10 Gbit/s = 10,000 Mbit/s.\n3. Calculation: 8 / 10,000 = 0.0008 seconds = 0.8 ms.\n4. Reality: Latency dominates. It will take **~1-2 ms**.", "rejected": "1MB / 10 Gbit/s = 0.1s = 100ms. (Unit error)."}
{"prompt": "How many 1KB 'logs' per day from 10k servers, 100 log/sec/server?", "chosen": "1. Servers: 10,000\n2. Rate: 100 logs/sec/server\n3. Total rate: 1,000,000 logs/sec.\n4. Size: 1 KB\n5. Data/sec: 1,000,000 KB/s = 1 GB/s.\n6. Data/day: 1 GB/s * 86,400 = 86,400 GB = 86.4 TB.\nWe'd get **86.4 TB/day**.", "rejected": "10,000 * 100 * 1KB = 1GB/day. (Ignores 'per sec')."}
{"prompt": "A 1PB 'DB'. 0.1% 'hot'. How much RAM to cache?", "chosen": "1. DB: 1 PB = 1,000 TB.\n2. Hot set: 0.1%.\n3. Calculation: 1,000 TB * 0.001 = 1 TB.\n4. Overhead: 25%. 1 * 1.25 = 1.25 TB.\nWe'd need **~1.25 TB** of RAM.", "rejected": "1PB * 0.1 = 100TB."}
{"prompt": "Estimate storage for 1B 'users' (1KB each) with 3x replication.", "chosen": "1. Users: 1,000,000,000\n2. Size: 1 KB\n3. Data: 1,000,000,000 * 1 KB = 1 TB.\n4. Replication: 3x.\n5. Total: 1 TB * 3 = 3 TB.\nWe'd need **3 TB** of total storage.", "rejected": "1B * 1KB = 1TB. (Ignores replication)."}
{"prompt": "How many 16-core servers for 3,200 RPS, if 1 core = 50 RPS?", "chosen": "1. Need: 3,200 RPS\n2. Server: 16 * 50 = 800 RPS.\n3. Servers: 3,200 / 800 = 4.\n4. Redundancy: 4 + 1 = 5.\nWe need **5 servers**.", "rejected": "3,200 / 50 = 64 cores. 64 / 16 = 4. 4 servers. (No redundancy)."}
{"prompt": "A 1M-user app. 5% churn/mo. How many users lost/year?", "chosen": "1. Users: 1,000,000\n2. Churn/mo: 5% = 50,000 users/mo.\n3. Churn/yr: 50,000 * 12 = 600,000.\nWe'd lose **~600,000 users/year**.", "rejected": "1M * 5% = 50,000. (Ignores 'per year')."}
{"prompt": "How many 1GB 'files' can 1EB hold?", "chosen": "1. Storage: 1 EB = 1,000,000,000 GB.\n2. File: 1 GB.\n3. Calculation: 1,000,000,000 / 1 = 1,000,000,000.\nIt can hold **1 billion files**.", "rejected": "1EB / 1GB = 1,000,000. (Unit error)."}
{"prompt": "A 1GB 'file'. How long to send over 1 Gbit/s?", "chosen": "1. Data: 1 GB = 8 Gbit.\n2. Speed: 1 Gbit/s.\n3. Calculation: 8 Gbit / 1 Gbit/s = 8 seconds.\nIt will take **8 seconds**.", "rejected": "1GB / 1Gbit/s = 1 second. (Confuses GB and Gbit)."}
{"prompt": "How many 1KB 'logs' per day from 10k servers, 1 log/day/server?", "chosen": "1. Servers: 10,000\n2. Rate: 1 log/day/server\n3. Total rate: 10,000 logs/day.\n4. Size: 1 KB\n5. Data/day: 10,000 * 1 KB = 10,000 KB = 10 MB.\nWe'd get **10 MB/day**.", "rejected": "10,000 * 1 * 1KB = 10GB/day. (Math error)."}
{"prompt": "A 10TB 'DB'. 1% 'hot'. How much RAM to cache?", "chosen": "1. DB: 10 TB.\n2. Hot set: 1%.\n3. Calculation: 10 TB * 0.01 = 0.1 TB = 100 GB.\n4. Overhead: 25%. 100 * 1.25 = 125 GB.\nWe'd need **~125 GB** of RAM.", "rejected": "10TB * 1% = 0.1GB. 100MB."}
{"prompt": "Estimate storage for 1M 'users' (1KB each) with 4x replication.", "chosen": "1. Users: 1,000,000\n2. Size: 1 KB\n3. Data: 1,000,000 * 1 KB = 1 GB.\n4. Replication: 4x.\n5. Total: 1 GB * 4 = 4 GB.\nWe'd need **4 GB** of total storage.", "rejected": "1M * 1KB = 1GB. (Ignores replication)."}
{"prompt": "How many 8-core servers for 4,000 RPS, if 1 core = 100 RPS?", "chosen": "1. Need: 4,000 RPS\n2. Server: 8 * 100 = 800 RPS.\n3. Servers: 4,000 / 800 = 5.\n4. Redundancy: 5 + 1 = 6.\nWe need **6 servers**.", "rejected": "4,000 / 100 = 40 cores. 40 / 8 = 5. 5 servers. (No redundancy)."}
{"prompt": "A 1M-user app. 10% churn/yr. How many users lost/mo?", "chosen": "1. Users: 1,000,000\n2. Churn/yr: 10% = 100,000 users/yr.\n3. Churn/mo: 100,000 / 12 = 8,333.\nWe'd lose **~8,333 users/month**.", "rejected": "1M * 10% = 100,000. (Gives per year)."}
{"prompt": "How many 1TB 'disks' can 1EB hold?", "chosen": "1. Storage: 1 EB = 1,000,000 TB.\n2. Disk: 1 TB.\n3. Calculation: 1,000,000 / 1 = 1,000,000.\nIt can hold **1 million disks**.", "rejected": "1EB / 1TB = 1,000. (Unit error)."}
{"prompt": "A 1TB 'file'. How long to send over 1 Gbit/s?", "chosen": "1. Data: 1 TB = 8 Tbit.\n2. Speed: 1 Gbit/s.\n3. Calculation: 8 Tbit / 1 Gbit/s = 8,000 seconds.\n4. Conversion: 8,000 / 3600 = 2.22 hours.\nIt will take **~2.2 hours**.", "rejected": "1TB / 1Gbit/s = 1,000 seconds. (Confuses TB and Tbit)."}
{"prompt": "How many 1KB 'logs' per day from 1 server, 1000 log/sec?", "chosen": "1. Servers: 1\n2. Rate: 1,000 logs/sec\n3. Size: 1 KB\n4. Data/sec: 1,000 KB/s = 1 MB/s.\n5. Data/day: 1 MB/s * 86,400 = 86,400 MB = 86.4 GB.\nWe'd get **86.4 GB/day**.", "rejected": "1 * 1000 * 1KB = 1MB/day. (Ignores 'per sec')."}
{"prompt": "A 100GB 'DB'. 50% 'hot'. How much RAM to cache?", "chosen": "1. DB: 100 GB.\n2. Hot set: 50%.\n3. Calculation: 100 GB * 0.50 = 50 GB.\n4. Overhead: 25%. 50 * 1.25 = 62.5 GB.\nWe'd need **~62.5 GB** of RAM.", "rejected": "100GB * 50 = 5000GB."}
{"prompt": "Estimate storage for 10M 'users' (1KB each) with 1x replication.", "chosen": "1. Users: 10,000,000\n2. Size: 1 KB\n3. Data: 10,000,000 * 1 KB = 10 GB.\n4. Replication: 1x (no replication).\n5. Total: 10 GB * 1 = 10 GB.\nWe'd need **10 GB**.", "rejected": "1x replication means 1 + 1 = 2 copies. 10 * 2 = 20GB."}
{"prompt": "How many 8-core servers for 8,000 RPS, if 1 core = 100 RPS?", "chosen": "1. Need: 8,000 RPS\n2. Server: 8 * 100 = 800 RPS.\n3. Servers: 8,000 / 800 = 10.\n4. Redundancy: 10 + 1 = 11.\nWe need **11 servers**.", "rejected": "8,000 / 100 = 80 cores. 80 / 8 = 10. 10 servers. (No redundancy)."}
{"prompt": "A 1M-user app. 12% churn/yr. How many users lost/mo?", "chosen": "1. Users: 1,000,000\n2. Churn/yr: 12% = 120,000 users/yr.\n3. Churn/mo: 120,000 / 12 = 10,000.\nWe'd lose **10,000 users/month**.", "rejected": "1M * 12% = 120,000. (Gives per year)."}
{"prompt": "How many 1GB 'files' can 10TB hold?", "chosen": "1. Storage: 10 TB = 10,000 GB.\n2. File: 1 GB.\n3. Calculation: 10,000 / 1 = 10,000.\nIt can hold **10,000 files**.", "rejected": "10TB / 1GB = 10. (Unit error)."}
{"prompt": "A 1MB 'file'. How long to send over 100 Mbit/s?", "chosen": "1. Data: 1 MB = 8 Mbit.\n2. Speed: 100 Mbit/s.\n3. Calculation: 8 / 100 = 0.08 seconds = 80 ms.\nIt will take **80 ms**.", "rejected": "1MB / 100Mbit/s = 0.01s = 10ms. (Confuses MB and Mb)."}
{"prompt": "How many 1KB 'logs' per day from 100 servers, 1 log/sec/server?", "chosen": "1. Servers: 100\n2. Rate: 1 log/sec/server\n3. Total rate: 100 logs/sec.\n4. Size: 1 KB\n5. Data/sec: 100 KB/s.\n6. Data/day: 100 KB/s * 86,400 = 8,640,000 KB = 8.64 GB.\nWe'd get **8.64 GB/day**.", "rejected": "100 * 1 * 1KB = 100KB/day. (Ignores 'per sec')."}
{"prompt": "A 1TB 'DB'. 10% 'hot'. How much RAM to cache?", "chosen": "1. DB: 1 TB.\n2. Hot set: 10%.\n3. Calculation: 1 TB * 0.10 = 0.1 TB = 100 GB.\n4. Overhead: 25%. 100 * 1.25 = 125 GB.\nWe'd need **~125 GB** of RAM.", "rejected": "1TB * 10% = 0.1GB. 100MB."}
{"prompt": "Estimate storage for 1M 'users' (10KB each) with 3x replication.", "chosen": "1. Users: 1,000,000\n2. Size: 10 KB\n3. Data: 1,000,000 * 10 KB = 10 GB.\n4. Replication: 3x.\n5. Total: 10 GB * 3 = 30 GB.\nWe'd need **30 GB**.", "rejected": "1M * 10KB = 10GB. (Ignores replication)."}
{"prompt": "How many 4-core servers for 800 RPS, if 1 core = 100 RPS?", "chosen": "1. Need: 800 RPS\n2. Server: 4 * 100 = 400 RPS.\n3. Servers: 800 / 400 = 2.\n4. Redundancy: 2 + 1 = 3.\nWe need **3 servers**.", "rejected": "800 / 100 = 8 cores. 8 / 4 = 2. 2 servers. (No redundancy)."}
{"prompt": "A 1M-user app. 10% churn/yr. How many users lost?", "chosen": "1. Users: 1,000,000\n2. Churn/yr: 10%.\n3. Calculation: 1,000,000 * 0.10 = 100,000.\nWe'd lose **100,000 users/year**.", "rejected": "1M * 10 = 10M. 10 million lost."}
{"prompt": "How many 1TB 'disks' can 1PB hold?", "chosen": "1. Storage: 1 PB = 1,000 TB.\n2. Disk: 1 TB.\n3. Calculation: 1,000 / 1 = 1,000.\nIt can hold **1,000 disks**.", "rejected": "1PB / 1TB = 1. (Unit error)."}
{"prompt": "A 1GB 'file'. How long to send over 100 Mbit/s?", "chosen": "1. Data: 1 GB = 8 Gbit = 8,000 Mbit.\n2. Speed: 100 Mbit/s.\n3. Calculation: 8,000 / 100 = 80 seconds.\nIt will take **80 seconds**.", "rejected": "1GB / 100Mbit/s = 0.01s = 10ms. (Confuses GB and Gbit)."}
{"prompt": "How many 1KB 'logs' per day from 1 server, 10 log/sec?", "chosen": "1. Servers: 1\n2. Rate: 10 logs/sec\n3. Size: 1 KB\n4. Data/sec: 10 KB/s.\n5. Data/day: 10 KB/s * 86,400 = 864,000 KB = 864 MB.\nWe'd get **864 MB/day**.", "rejected": "1 * 10 * 1KB = 10KB/day. (Ignores 'per sec')."}